{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d350971c",
   "metadata": {},
   "source": [
    "# Welcome to Versatile Data Kit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575951ae",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to our introductory guide. \n",
    "Here, we'll walk you through the process of creating, developing, and deploying a Data job using the Jupyter UI. \n",
    "Please take the time to carefully go through this guide to ensure a smooth and effective understanding on using VDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb565197",
   "metadata": {},
   "source": [
    "## Step 0: Explore VDK's Functionalities\n",
    "You can execute any shell command if you prefix it with \"!\". \"!vdk --help\" gives you good knowledge on VDK's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b972ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad33190-232c-48b7-a62b-4d881a0a9b1c",
   "metadata": {},
   "source": [
    "These commands are also readily accessible in the VDK menu located on the top toolbar. \n",
    "This guide will delve into each of these options for a comprehensive understanding.\n",
    "We'll also provide you with information on utilizing VDK within Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e038188",
   "metadata": {},
   "source": [
    "## Step 1 Login (Might be omitted)\n",
    "\n",
    "TO BE ADDED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b374e65",
   "metadata": {},
   "source": [
    "## Step 2: [Option 1] Create a Data Job\n",
    "Now that we have explored VDK's capabilities, let's create our data job (if you want to update an existing job see <i>\"Step 2: [Option 2]\"</i>). \n",
    "\n",
    "Go to the VDK menu it the tool bar and click the Create button the you will see some inputs: \n",
    "* name:  The data job name. It must be between 6 and 45 characters: lowercase or dash.\n",
    "* team: The team name to which the job should belong to.\n",
    "* path: Realative Jupyter path to the parent directory of the DataJob.\n",
    "\n",
    "If none specified it will use the Jupyter root directory. \n",
    "New directory using job name will be created inside that\n",
    "path with sample job. \n",
    "\n",
    "\n",
    "Once you've finished entering the inputs, proceed by clicking the \"Ok\" button.\n",
    "Shortly after, an alert will update you on the operation's status. \n",
    "If this takes longer than expected, you have the option to check the status manually using the \"Check Status\" button located on the right side of the toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f22b14-edf3-451a-a2be-1d1ec14ec893",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font color='red'>**ATTENTION!**</font>  Please be aware that VDK will attempt to create the Data Job on your local system and in the cloud, provided there's a designated REST API URL for the Control Service.\n",
    "You'll receive relevant updates clarifying whether the job has been created just on your local system, or in both local and cloud environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86556f57",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font> Should you encounter difficulties while creating a job, consider referencing our YouTube tutorial: 'Example Tutorial'(add link to tutorial which includes creating a job from Jupyter UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504372c",
   "metadata": {},
   "source": [
    "Once the operation is complete, feel free to navigate through the newly created data job directory.\n",
    "\n",
    "> _Data Job directory can contain any files, however, there are some files that are treated in a specific way:_\n",
    "> \n",
    "> * **SQL files (.sql)** - called SQL steps - are directly executed as queries against a database.\n",
    "> * **Notebook files (.ipynb)** -  can contain labelled VDK cells(called notebook steps) which are executed as steps in a VDK Data Job;\n",
    "> * **Python files (.py)** - called Python steps - are python scripts that define a run function that takes as an argument the job_input object.\n",
    "> * **config.ini** is needed in order to schedule a job.\n",
    "> * **requirements.txt** is needed when your Python steps use external python libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e8a16",
   "metadata": {},
   "source": [
    "## Step 2: [Option 2] Download a Data Job\n",
    "Should you wish to update an existing data job, this task can certainly be accomplished! The VDK menu, located on the toolbar, provides the necessary functionality to download the job.\n",
    "\n",
    "Simply navigate to the \"Download Job\" option within the menu and provide the requested inputs:\n",
    "\n",
    "* name:  The data job name. It must be between 6 and 45 characters: lowercase or dash.\n",
    "* team: The team name to which the job should belong to.\n",
    "* path: Relative Jupyter path to the parent directory where the Data Job will be downloaded.\n",
    "\n",
    "If none specified it will use the Jupyter root directory. \n",
    "\n",
    "Upon completion of the input fields, click the \"Ok\" button to advance. \n",
    "Shortly after, an alert will update you on the operation's status. \n",
    "If this takes longer than expected, you have the option to check the status manually using the \"Check Status\" button located on the right side of the toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c20c7-ece9-4a78-adea-de4ff474725c",
   "metadata": {},
   "source": [
    "## Step 3: Develop Data Job (in the notebok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeac937-04bb-4e46-89b1-e946406e5cd3",
   "metadata": {},
   "source": [
    "### What exactly is a Data Job?\n",
    "Data job is a data processing unit that allows data engineers to implement automated pull ingestion (E in ELT) or batch data transformation into Data Warehouse (T in ELT). At the core of it, it is a directory with different scripts and inside of it.\n",
    "> _Data Job directory can contain any files, however, there are some files that are treated in a specific way:_\n",
    "> \n",
    "> * **SQL files (.sql)** - called SQL steps - are directly executed as queries against a database.\n",
    "> * **Notebook files (.ipynb)** -  can contain labelled VDK cells(called notebook steps) which are executed as steps in a VDK Data Job;\n",
    "> * **Python files (.py)** - called Python steps - are python scripts that define a run function that takes as an argument the job_input object.\n",
    "> * **config.ini** is needed in order to schedule a job.\n",
    "> * **requirements.txt** is needed when your Python or Notebook steps use external python libraries.\n",
    "\n",
    "VDK supports having many Python, SQL and Notebook steps in a single Data Job. \n",
    "These steps are carried out in an ascending alphabetical sequence based on the file names, and Notebook steps are run from the beginning to the end of the file. For an effective execution order while retaining informative file names, it's beneficial to prefix your file names with numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e286e3-17fd-41dc-be23-77f8ce96a65e",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font> \n",
    "If this is your initial experience developing a notebook job, we highly recommend starting with our tutorial: \"How to Build a Job Using VDK Notebook Integration.\"(youtube video link to be added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a3464-7de6-4ec1-a20f-6df9633c157d",
   "metadata": {},
   "source": [
    "#### Notebook steps\n",
    "As previously stated, a single notebook can comprise multiple Data Job steps - these are distinct code segments executed by VDK. Within the notebook, an universal VDK variable named 'job_input' is provided. For more information, please refer to the following link: https://github.com/vmware/versatile-data-kit/blob/8c8b752a1f4400841331b2dda42cc1b6ef7a61af/projects/vdk-core/src/vdk/api/job_input.py#L355. It's entirely feasible to develop a Data Job within the notebook utilizing any cell, but it's important to bear in mind some  guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9ae5e-2649-4655-96d0-efe6ed9880ff",
   "metadata": {},
   "source": [
    "##### Execution Order and Identifying Cells\n",
    "*  *Cells can be labeled with the \"vdk\" tag.*\n",
    "*  *While this tag doesn't impact the data job's development phase, it is crucial for subsequent automated operations, such as a \"VDK Run.\"*\n",
    "*  *Only the cells critical to the Data Job should be tagged.*\n",
    "*  *Untagged cells will be overlooked during deployment and other execution processes.*\n",
    "*  *You can easily recognize VDK cells by their unique color scheme, the presence of the VDK logo, and an exclusive numbering system.*\n",
    "*  *VDK cells in the notebook will be executed according to the numbering when executing the notebook data job with VDK.*\n",
    "*  *You can delete the cells that are not tagged with \"vdk\" \n",
    "    as they are not essential to the data job's execution.\n",
    "    However, removing VDK cells will result in a different data job execution.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb639267-1527-4fef-ac33-647cfd3c9d5f",
   "metadata": {},
   "source": [
    "##### Tips: \n",
    "* *Before running the job, it is recommended to review the cells\n",
    "    to ensure a clear understanding of the data job run.  \n",
    "    This will help to ensure the desired outcome.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e73d96",
   "metadata": {},
   "source": [
    "## Step 4: Run Data Job locally (in the notebok)\n",
    "\n",
    "You can run data job locally in the notebook using 'Run' option from the VDK menu.\n",
    "After navigating to the option provide the requested inputs:\n",
    "\n",
    "* path: relative Jupyter path to the DataJob directory\n",
    "* arguments: Pass arguments. Those arguments will be passed to each step. Must be in valid JSON format. Arguments are passed to each step. They can be used as parameters in SQL queries and will be replaced automatically.Properties can also be specified as parameters in SQL, arguments would have higher priority for the same key.\n",
    "\n",
    "If none specified it will use the Jupyter root directory. \n",
    "\n",
    "Upon completion of the input fields, click the \"Ok\" button to advance. \n",
    "Shortly after, an alert will update you on the operation's status. \n",
    "If this takes longer than expected, you have the option to check the status manually using the \"Check Status\" button located on the right side of the toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e81c9-5796-4049-ab03-fc2accf22bb9",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font> To access the detailed logs of the Run operation, you can proceed to the job's parent directory. There, you'll find a file named 'vdk_logs'. This file contains the comprehensive logs associated with the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564922b3",
   "metadata": {},
   "source": [
    "## Step 5: Schedule data job \n",
    "\n",
    "All data jobs must contain a job configuration file called config.ini \n",
    "You can edit the schedule of the job by editing `schedule_cron` option. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf0ee4",
   "metadata": {},
   "source": [
    "## Step 6: Deploy data job\n",
    "\n",
    "Now that we are done with the modifications to the Data Job, we will deploy it in the Control Service.\n",
    "Deployment takes both the build/code and the deployment-specific properties, builds and packages them and once a Data Job is deployed, it is ready for immediate execution in the execution environment. It can be scheduled to run periodically.\n",
    "\n",
    "To deploy a Data Job, simply proceed to the \"Deploy\" option within the VDK menu and  provide the requested inputs: \n",
    "* name:  The data job name. It must be between 6 and 45 characters: lowercase or dash.\n",
    "* team: The team name to which the job should belong to.\n",
    "* path: Realative Jupyter path to the parent directory of the DataJob.\n",
    "* reason: Mandatory input contaning the reason for the deployment.\n",
    "\n",
    "Upon completion of the input fields, click the \"Ok\" button to advance.\n",
    "Shortly after, an alert will update you on the operation's status. \n",
    "This will submit the code of the Data Job to the Control Service and will create a Data Job Deployment. \n",
    "The Deployment process is asynchronous and even though the command completes fast, the creation takes a while until the Data Job is deployed and ready for execution.\n",
    "If you have configured `notified_on_job_deploy` in `config.ini` of the Data Job, you will get mail notification.\n",
    "You can validate that the Data Job Deployment is completed by checking VDK's Operations UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c933b0d-5242-4bb0-8270-c1281a7b57c2",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION!**</font> \n",
    "If this is your initial experience deploying a notebook job, we highly recommend starting with our tutorial: \"How to deploy and configure job using notebook\"(youtube video link to be added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
