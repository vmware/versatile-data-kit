{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d350971c",
   "metadata": {},
   "source": [
    "# Welcome to Versatile Data Kit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69a8b8-e573-4f31-ab59-cbc312789e9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [What exactly is a Data Job?](#data-job)\n",
    "- [Step 1: Login](#login)\n",
    "- [Step 2: [Option 1] Create a Data Job](#create)\n",
    "- [Step 2: [Option 2] Download a Data Job](#download)\n",
    "- [Step 3: Develop Data Job](#develop)\n",
    "- [Step 4: Run Data Job locally](#run)\n",
    "- [Step 5: Schedule data job](#schedule)\n",
    "- [Step 6: Deploy data job](#deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575951ae",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to our introductory guide. \n",
    "Here, we'll walk you through the process of creating, developing, and deploying a Data job using the Jupyter UI. \n",
    "Please take the time to carefully go through this guide to ensure a smooth and effective understanding on using VDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeac937-04bb-4e46-89b1-e946406e5cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What exactly is a Data Job? <a name=\"data-job\"></a>\n",
    "\n",
    "<p>Data job is a data processing unit that allows data engineers to implement automated pull ingestion (E in ELT) or batch data transformation into Data Warehouse (T in ELT). A'At the core of it, it is a directory with different scripts and inside of it.</p>\n",
    "\n",
    "<h4>Data Job directory can contain any files, however, there are some files that are treated in a specific way.</h4>\n",
    "\n",
    " \n",
    "<details>\n",
    "        <summary><strong style=\"color: Tomato;\">Click to see them!</strong></summary>\n",
    "        <ul>\n",
    "            <li><strong style=\"color: blue;\">SQL files (.sql)</strong> - called SQL steps - are directly executed as queries against a database.</li>\n",
    "            <li><strong style=\"color: green;\">Notebook files (.ipynb)</strong> - can contain labelled VDK cells (called notebook steps) which are executed as steps in a VDK Data Job;</li>\n",
    "            <li><strong style=\"color: red;\">Python files (.py)</strong> - called Python steps - are python scripts that define a run function that takes as an argument the job_input object.</li>\n",
    "            <li><strong style=\"color: purple;\">config.ini</strong> is needed in order to schedule a job.</li>\n",
    "            <li><strong style=\"color: brown;\">requirements.txt</strong> is needed when your Python or Notebook steps use external python libraries.</li>\n",
    "        </ul>\n",
    "        <img src=\"./images/data-job-directory.png\" width=\"1000\" length=\"546\" alt=\"Data Job Directory\">\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "   <p>VDK supports having many Python, SQL and Notebook steps in a single Data Job. These steps are carried out in an ascending alphabetical sequence based on the file names, and Notebook steps are run from the beginning to the end of the file. For an effective execution order while retaining informative file names, it's beneficial to prefix your file names with numbers.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e038188",
   "metadata": {},
   "source": [
    "## Step 1: Login <a name=\"login\"></a>\n",
    "\n",
    "<p> First you need to login in order to be able to create and deploy jobs.</p> \n",
    "<img src=\"./images/login.png\" width=\"1000\" length=\"546\" alt=\"Login\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b374e65",
   "metadata": {},
   "source": [
    "## Step 2: [Option 1] Create a Data Job <a name=\"create\"></a>\n",
    "\n",
    "\n",
    "<p>Now that we have explored VDK's capabilities, let's create our data job (if you want to update an existing job see <i>\"Step 2: [Option 2]\"</i>). </p>\n",
    "\n",
    "<img src=\"./images/create.png\" width=\"1000\" length=\"546\" alt=\"Create a Data Job\">\n",
    "\n",
    "<p>If none specified it will use the Jupyter root directory. \n",
    "New directory using job name will be created inside that\n",
    "path with sample job. </p>\n",
    "\n",
    "<font color='green'>**WARNING!**</font> Should you encounter difficulties while creating a job, consider referencing our tutorial: 'Example Tutorial'(add link to tutorial which includes creating a job from Jupyter UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e8a16",
   "metadata": {},
   "source": [
    "## Step 2: [Option 2] Download a Data Job <a name=\"download\"></a>\n",
    "\n",
    "\n",
    "<img src=\"./images/download.png\" width=\"1000\" length=\"546\" alt=\"Download Job\">\n",
    "\n",
    "<p>If none specified it will use the Jupyter root directory. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885befcd-2e4d-4896-bf5a-4aedf80244ae",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong style=\"color: Tomato;\">Convert Job To Notebook [Optional]</strong></summary>\n",
    "    <p>After downloading a job you can convert it to a notebook job from directory type job.</p>\n",
    "    <img src=\"./images/convert.png\" width=\"1000\" length=\"546\" alt=\"Convert Job\">\n",
    "    <p>In this way you can work with notebooks instead of .py and .sql files. To learn more about how to work with notebook steps check <i>Step 3</i>.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c20c7-ece9-4a78-adea-de4ff474725c",
   "metadata": {},
   "source": [
    "## Step 3: Develop Data Job <a name=\"develop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e286e3-17fd-41dc-be23-77f8ce96a65e",
   "metadata": {},
   "source": [
    "<font color='green'>**TIP!**</font>\n",
    "If this is your initial experience developing a notebook job, we highly recommend starting with our tutorial: \"How to Build a Job Using VDK Notebook Integration.\"(tutorial link to be added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea3a74-f895-4baa-9bf5-04fee34ac560",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong style=\"color: DarkRed;\">Python steps</strong></summary>\n",
    "    <p>This is the structure of a Python step:</p>\n",
    "    \n",
    "<pre style=\"background-color: #f5f5f5; padding: 15px; border: 1px solid #ccc; border-radius: 5px;\"><code>\n",
    "def run(job_input: IJobInput):\n",
    "    ... gather data ...\n",
    "    job_input.send_object_for_ingestion(data_to_ingest, \"table_from_the_data_lake_to_ingest_into\")\n",
    "</code></pre>\n",
    "    \n",
    "</br>\n",
    "<p>For every Python script, VDK provides an object - job_input, that has methods for:</p>\n",
    "<ul>\n",
    "        <li>Executing queries</li>\n",
    "        <li>Ingesting data into the Data Lake</li>\n",
    "        <li>Integrating Data Lake data into a dimensional model in Data Warehouse</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b1530-4f17-4847-b9d5-0aea3f91cbc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "    <summary><strong style=\"color: Blue;\">SQL steps</strong></summary>\n",
    "    <p>SQL scripts are standard SQL scripts.</p>\n",
    "    \n",
    "<p>Common uses of SQL steps are:</p>\n",
    "    <ul>\n",
    "        <li>Aggregating data from other tables to a new one</li>\n",
    "        <li>Creating a table or a view that is needed for the Python steps</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9ae5e-2649-4655-96d0-efe6ed9880ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong style=\"color: Green;\">Notebook Steps</summary>\n",
    "    <p>A single notebook can comprise multiple Data Job steps - these are distinct code segments executed by VDK. Within the notebook,VDK provides an object - job_input, that has methods for:</p>\n",
    "<ul>\n",
    "        <li>Executing queries</li>\n",
    "        <li>Ingesting data into the Data Lake</li>\n",
    "        <li>Integrating Data Lake data into a dimensional model in Data Warehouse</li></p>\n",
    "    \n",
    "<h4>Execution Order and Identifying Cells</h4>\n",
    "    <ul>\n",
    "        <li>Cells can be labeled with the \"vdk\" tag.</li>\n",
    "        <li>While this tag doesn't impact the data job's development phase, it is crucial for subsequent automated operations, such as a \"VDK Run.\"</li>\n",
    "        <li>Only the cells critical to the Data Job should be tagged.</li>\n",
    "        <li>Untagged cells will be overlooked during deployment and other execution processes.</li>\n",
    "        <li>You can easily recognize VDK cells by their unique color scheme, the presence of the VDK logo, and an exclusive numbering system.</li>\n",
    "        <li>VDK cells in the notebook will be executed according to the numbering when executing the notebook data job with VDK.</li>\n",
    "        <li>You can delete the cells that are not tagged with \"vdk\" as they are not essential to the data job's execution. However, removing VDK cells will result in a different data job execution.</li>\n",
    "    </ul>\n",
    "        </br>\n",
    "<img src=\"./images/cell-tags.png\" width=\"1000\" length=\"546\" alt=\"Cell tags\">\n",
    "    \n",
    "<h5>Tips:</h5>\n",
    "    <ul>\n",
    "        <li>Before running the job, it is recommended to review the cells to ensure a clear understanding of the data job run. This will help to ensure the desired outcome.</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e73d96",
   "metadata": {},
   "source": [
    "## Step 4: Run Data Job locally <a name=\"run\"></a>\n",
    "\n",
    "<img src=\"./images/run.png\" width=\"1000\" length=\"546\" alt=\"Run a job\">\n",
    "\n",
    "<p>If none specified it will use the Jupyter root directory. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564922b3",
   "metadata": {},
   "source": [
    "## Step 5: Schedule data job <a name=\"schedule\"></a>\n",
    "\n",
    "<p>All data jobs must contain a job configuration file called config.ini \n",
    "You can edit the schedule of the job by editing `schedule_cron` option. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf0ee4",
   "metadata": {},
   "source": [
    "## Step 6: Deploy data job <a name=\"deploy\"></a>\n",
    "\n",
    "<p>Now that we are done with the modifications to the Data Job, we will deploy it in the Control Service.\n",
    "Deployment takes both the build/code and the deployment-specific properties, builds and packages them and once a Data Job is deployed, it is ready for immediate execution in the execution environment. It can be scheduled to run periodically.</p>\n",
    "\n",
    "<img src=\"./images/deploy.png\" width=\"1000\" length=\"546\" alt=\"Deploy a job\">\n",
    "\n",
    "<p>The Deployment process is asynchronous and even though the command completes fast, the creation takes a while until the Data Job is deployed and ready for execution.\n",
    "If you have configured `notified_on_job_deploy` in `config.ini` of the Data Job, you will get mail notification.\n",
    "You can validate that the Data Job Deployment is completed by checking VDK's Operations UI.</p>\n",
    "\n",
    "<font color='green'>**TIP!**</font>\n",
    "If this is your initial experience deploying a notebook job, we highly recommend starting with our tutorial: \"How to deploy and configure job using notebook\"(youtube video link to be added)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
