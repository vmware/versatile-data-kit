# The file contains default configuration used in both production and development setup.

# Used for log file name
taurus.svc.name=data-jobs
# http://localhost:8080/${svc.url.prefix}/debug/servertime
sccp.svc.url.prefix=data-jobs
# http://localhost:8080/${svc.url.prefix}/debug
management.endpoints.web.base-path=/data-jobs/debug
management.endpoints.web.exposure.include=*

datajobs.version=0.0.1
springdoc.swagger-ui.path=/${taurus.svc.name}/swagger-ui.html
springdoc.api-docs.path=/${taurus.svc.name}/api-docs
spring.profiles.active=dev

# When updating or adding ports for services, these changes must be made in many places:
# 1) In each of the values.yml files at /k8s/envs/*/values.yml
# 2) In each service's application.properties at /services/*/src/main/resources/application.*
# 3) In base's service discovery enum at /libraries/base/src/main/java/com/vmware/taurus/discovery/ServiceLocation.java
server.port=8092

# We use Flyway (https://flywaydb.org/) for explicit database schema definition and migrations, so we disable Hibernate
# automated schema management. The automated management would be too opaque.
# The explicit migrations are defined in /src/main/resources/db.migration.
spring.jpa.hibernate.ddl-auto=none

# show-sql doesn't use logging but just prints to stdout
# to see SQL statements via logging enable DEBUG level on logger org.hibernate.SQL
# to see parameter values of SQL statements enable TRACE level on logger org.hibernate.type.descriptor.sql.BasicBinder
# NB: you can use a spring or JVM property in the form - logging.level.LOGGER_NAME=LEVEL - to override a level
# or environment variable LOGGING_LEVEL_LOGGER_NAME=LEVEL
# See https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-custom-log-levels
spring.jpa.show-sql=false
spring.jpa.properties.hibernate.show_sql=false
logging.level.org.hibernate.SQL=INFO
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=INFO
logging.level.com.vmware.taurus.service.locks.CustomLockProvider=INFO


spring.jpa.open-in-view=false


# Security configuration. Note if you enable security, you may
# also enable authentication provider(s) and authorization
# featureflag.security.enabled=true

# Settings for Kerberos authentication provider
datajobs.security.kerberos.enabled=${SECURITY_KERBEROS_ENABLED:false}
# Properties mandatory if datajobs.security.kerberos.enabled=true
datajobs.security.kerberos.kerberosPrincipal=${SECURITY_KERBEROS_SERVICE_PRINCIPAL:""}
datajobs.security.kerberos.keytabFileLocation=${SECURITY_KERBEROS_SERVICE_KEYTAB_FILE_LOCATION:}
datajobs.security.kerberos.krb5ConfigLocation=${SECURITY_KERBEROS_KRB5_CONFIG_LOCATION:}

# Settings for OAuth2 authentication provider
datajobs.security.oauth2.enabled=true

# Configuration persistence are the persisted properties that allow the seamless migration
# of data jobs to a new K8S cluster. CSV separated list. Current supported values are K8S and DB.
datajobs.deployment.configuration.persistence.writeTos=${DATAJOBS_DEPLOYMENT_CONFIGURATION_PERSISTENCE_WRITE_TOS:"K8S,DB"}
# Variable to select the truth data source for reading properties. Options are "DB" for database and
# "K8S" for kubernetes.
datajobs.deployment.configuration.persistence.readDataSource=${DATAJOBS_DEPLOYMENT_CONFIGURATION_PERSISTENCE_READ_DATA_SOURCE:"K8S"}

# The data job deployments' synchronization task enabled
datajobs.deployment.configuration.synchronization.task.enabled=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_ENABLED:false}

# The data job deployments' synchronization task interval is the time period (expressed in milliseconds)
# after a synchronization process has completed and before a new one is started. The time unit is milliseconds.
datajobs.deployment.configuration.synchronization.task.interval.ms=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_INTERVAL_MS:60000}
# The data job deployments' synchronization task initial delay is the period (expressed in milliseconds) between control service
# start and the first time a synchronization process is started by the control service instance. The time unit is milliseconds.
datajobs.deployment.configuration.synchronization.task.initial.delay.ms=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_INITIAL_DELAY_MS:10000}
# The core pool size of the ThreadPoolExecutor for synchronizing data job deployments.
datajobs.deployment.configuration.synchronization.task.corePoolSize=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_CORE_POOL_SIZE:5}
# The maximum pool size of the ThreadPoolExecutor for synchronizing data job deployments.
datajobs.deployment.configuration.synchronization.task.maxPoolSize=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_MAX_POOL_SIZE:5}
# The queue capacity of the ThreadPoolExecutor for synchronizing data job deployments.
datajobs.deployment.configuration.synchronization.task.queueCapacity=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_QUEUE_CAPACITY:4000}
# The keep alive seconds of the ThreadPoolExecutor for synchronizing data job deployments.
datajobs.deployment.configuration.synchronization.task.keepAliveSeconds=${DATAJOBS_DEPLOYMENT_CONFIGURATION_SYNCHRONIZATION_TASK_KEEP_ALIVE_S:120}

# The JSON Web Key Set (JWKS) is a set of keys which contains the public keys
# used to verify any JSON Web Token (JWT) issued by the authorization server
# It is required.
spring.security.oauth2.resourceserver.jwt.jwk-set-uri=${SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK_SET_URI:http://localhost}

# This property is deprecated in favor of the issuer.uris and will be removed in the future.
#spring.security.oauth2.resourceserver.jwt.issuer-uri=${SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUER_URI}
# Comma separated list of issuer uris.
#spring.security.oauth2.resourceserver.jwt.issuer.uris=${SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUER_URIS:}

# Authorization configuration. Note if you enable authorization you should
# also point the webhook endpoint against which authorization is delegated
# in order for the feature to fully work
featureflag.authorization.enabled=false
datajobs.authorization.webhook.endpoint=
datajobs.authorization.webhook.authentication.enabled=false
datajobs.authorization.webhook.authorization.server.endpoint=
datajobs.authorization.webhook.authorization.refresh.token=
datajobs.authorization.jwt.claim.username=username


# Data Jobs post webhook settings (Create and Delete)
datajobs.post.create.webhook.endpoint=
datajobs.post.create.webhook.internal.errors.retries=3
datajobs.post.create.webhook.authentication.enabled=false
datajobs.post.create.webhook.authorization.server.endpoint=
datajobs.post.create.webhook.authorization.refresh.token=
datajobs.post.delete.webhook.endpoint=
datajobs.post.delete.webhook.internal.errors.retries=3
datajobs.post.delete.webhook.authentication.enabled=false
datajobs.post.delete.webhook.authorization.server.endpoint=
datajobs.post.delete.webhook.authorization.refresh.token=
datajobs.create.oauth.app.webhook.endpoint=
datajobs.create.oauth.app.webhook.internal.errors.retries=3
datajobs.create.oauth.app.webhook.authentication.enabled=false
datajobs.create.oauth.app.webhook.authorization.server.endpoint=
datajobs.create.oauth.app.webhook.authorization.refresh.token=

# The owner name and email address that will be used to send all Versatile Data Kit related email notifications.
datajobs.notification.owner.email=versatiledatakit@groups.vmware.com
datajobs.notification.owner.name=Versatile Data Kit
# datajobs.notification.cc.emails=cc1@x.com,cc2@x.com

# The gitlab repository and credentials for pulling data jobs code when building their images.
datajobs.git.url=${GIT_URL}
datajobs.git.username=${GIT_USERNAME}
datajobs.git.password=${GIT_PASSWORD}
datajobs.git.branch=${GIT_BRANCH:master}
datajobs.git.url.pattern=${GIT_URL_PATTERN:/tree/%s/%s}
datajobs.git.remote=${GIT_REMOTE:origin}
datajobs.git.ssl.enabled=${GIT_SSL_ENABLED:true}
# The registry type, if left blank, defaults to ecr. The alternative registry type is generic
# which means the registry is of the Harbor or Dockerhub type.
datajobs.docker.registryType=generic

# Docker repository used to store data job images
datajobs.docker.repositoryUrl=ghcr.io/versatile-data-kit-dev/dp
datajobs.docker.registrySecret=${DOCKER_REGISTRY_SECRET:}

# Docker repository used to store VDK SDK image
datajobs.vdk.docker.registrySecret=${VDK_SDK_DOCKER_REGISTRY_SECRET:}

# open API generator is not generating example for parameters and causing noisy warnings
logging.level.io.swagger.models.parameters.AbstractSerializableParameter=ERROR


datajobs.control.k8s.kubeconfig=${HOME}/.kube/config
datajobs.control.k8s.jobTTLAfterFinishedSeconds=3600
# Location to a K8s cronjob yaml file which will be used as a template
# for all data jobs. If the location is missing, the default internal
# cronjob yaml resource will be used (see k8s-data-job-template.yaml).
# Note that this location is expected to be set via the environment
# variable '# variable 'K8S_DATA_JOB_TEMPLATE_FILE'.'.
datajobs.control.k8s.data.job.template.file=${K8S_DATA_JOB_TEMPLATE_FILE:#{null}}

datajobs.monitoring.sync.interval=5000
datajobs.monitoring.sync.initial.delay=10000

# The status watch interval is the time period (expressed in milliseconds) after a status
# watch operation has completed and before a new one is started
datajobs.status.watch.interval=1000
# The status watch initial delay is the period (expressed in milliseconds) between control service
# start and the first time a data job status watch is started by the control service instance
datajobs.status.watch.initial.delay=10000


# The base image which will be used to create the image where data job would be run
# On top of it the job source and its dependencies are installed for each job
datajobs.deployment.dataJobBaseImage=python:3.9-slim

# The map of python version and respective data job base and vdk images that would be
# used for data job deployments
datajobs.deployment.supportedPythonVersions=${DATAJOBS_DEPLOYMENT_SUPPORTED_PYTHON_VERSIONS:{3.9: {vdkImage: 'registry.hub.docker.com/versatiledatakit/quickstart-vdk:release', baseImage: 'python:3.9-slim'}}}
# The default python version, which is to be used when selecting the vdk and job base images from
# the supportedPythonVersions for data job deployments
datajobs.deployment.defaultPythonVersion=${DATAJOBS_DEPLOYMENT_DEFAULT_PYTHON_VERSION:3.9}

#Configuration variables used for data job execution cleanup
#This is a spring cron expression, used to schedule the clean up job / default is every 3 hours
datajobs.executions.cleanupJob.scheduleCron=0 0 */3 * * *
#This variable is used to determine the maximum amount of executions the database can store / default is 100
#exposed in helm chart, if there are more executions than maximum, older ones will be deleted when clean up job runs.
datajobs.executions.cleanupJob.maximumExecutionsToStore=${DATAJOBS_EXECUTION_MAXIMUM_EXECUTIONS_TO_STORE:100}
#This variable exposes the total time to live of data job execution in seconds / default is 14 days
#executions older than that will get deleted when the clean up job runs
datajobs.executions.cleanupJob.executionsTtlSeconds=${DATAJOBS_EXECUTION_TTL_SECONDS:1209600}

# This template will be used for building of logs URL for each data job execution returned by API.
# Supported variables which will be replaced in the template with the particular execution values:
# {execution_id}, {job_name}, {op_id}, {start_time} and {end_time}
# Example: "https://log-insight-url/li/query/stream?query=%C2%A7%C2%A7%C2%A7AND%C2%A7%C2%A7%C2%A7%C2%
# A7{{start_time}}%C2%A7{{end_time}}%C2%A7true%C2%A7COUNT%C2%A7text:CONTAINS:{{execution_id}}*"
datajobs.executions.logsUrl.template=${DATAJOBS_EXECUTIONS_LOGS_URL_DATE_TEMPLATE:}

# This variable is used to determine the format of execution.startTime
# and execution.endTime used in the template (datajobs.executions.logsUrl.template).
# Supported options: iso (https://bg.wikipedia.org/wiki/ISO_8601) and unix (https://en.wikipedia.org/wiki/Unix_time).
# If left blank, defaults to unix.
# Examples: iso -> "2021-12-03T15:34:54.822098Z", unix -> "1638545973226".
datajobs.executions.logsUrl.dateFormat=${DATAJOBS_EXECUTIONS_LOGS_URL_DATE_FORMAT:unix}

# The offset (expressed in seconds) that will be added to or subtracted from the {{start_time}}
# variable during the building of logs URL. It could be either positive or negative number.
# In case of positive number it will be added to the {{start_time}}.
# In case of negative number it will be subtracted from the {{start_time}}.
# If left blank, defaults to 0.
datajobs.executions.logsUrl.startTimeOffsetSeconds=${DATAJOBS_EXECUTIONS_LOGS_URL_START_TIME_OFFSET_SECONDS:0}

# The offset (expressed in seconds) that will be added to or subtracted from the {{end_time}}
# variable during the building of logs URL. It could be either positive or negative number.
# In case of positive number it will be added to the {{end_time}}.
# In case of negative number it will be subtracted from the {{end_time}}.
# If left blank, defaults to 0.
datajobs.executions.logsUrl.endTimeOffsetSeconds=${DATAJOBS_EXECUTIONS_LOGS_URL_END_TIME_OFFSET_SECONDS:0}

# https://javaee.github.io/javamail/docs/api/com/sun/mail/smtp/package-summary.html
mail.transport.protocol= ${MAIL_TRANSPORT_PROTOCOL:smtp}
mail.smtp.host=${MAIL_SMTP_HOST:smtp.vmware.com}
# Fill these in if the notification emails need to go through an authenticated email.
mail.smtp.auth=${MAIL_SMTP_AUTH:false}
mail.smtp.starttls.enable=${MAIL_SMTP_STARTTLS_ENABLE:false}
mail.smtp.user=${MAIL_SMTP_USER:}
mail.smtp.password=${MAIL_SMTP_PASSWORD:}
mail.smtp.ssl.protocols= ${MAIL_SMTP_SSL_PROTOCOLS:TLSv1.2}
# Default port is 25 for unathenticated, 587 for authenticated
mail.smtp.port=${MAIL_SMTP_PORT:25}

# Set imagePullPolicy of the job-builder image
# Correspond to those defined by kubernetes
# See https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
datajobs.deployment.builder.imagePullPolicy=IfNotPresent


datajobs.deployment.builder.securitycontext.runAsUser=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_RUN_AS_USER:0}
datajobs.deployment.builder.securitycontext.runAsGroup=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_RUN_AS_GROUP:1000}
datajobs.deployment.builder.securitycontext.fsGroup=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_FS_GROUP:1000}

datajobs.deployment.builder.serviceAccountName=${DATAJOBS_DEPLOYMENT_BUILDER_SERVICE_ACCOUNT_NAME:}

datajobs.deployment.readOnlyRootFilesystem=${DATAJOBS_READ_ONLY_ROOT_FILESYSTEM:false}
# Sets imagePullPolicy of the JobImageDeployer
# this sets the imagePullPolicy of deployed data jobs in K8S
datajobs.deployment.jobImagePullPolicy=${DATAJOBS_DEPLOYMENT_JOB_IMAGE_PULL_POLICY:IfNotPresent}

datajobs.deployment.initContainer.resourreadOnlyRootFilesystem=${DATAJOBS_READ_ONLY_ROOT_FILESYSTEM:false}

# Resources set on Data Job initContainer
datajobs.deployment.initContainer.resources.requests.memory=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_REQUESTS_MEMORY:100Mi}
datajobs.deployment.initContainer.resources.requests.cpu=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_REQUESTS_CPU:100m}
datajobs.deployment.initContainer.resources.limits.memory=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_LIMITS_MEMORY:100Mi}
datajobs.deployment.initContainer.resources.limits.cpu=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_LIMITS_CPU:100m}


# Used to specify in which folders data jobs should be stored temporality while uploading/deleting or deploying.
# if this is not specified if will use a temp folder as the location. It will be a sub folder of the folder specified by `java.io.tmpdir`.
# the main purpose of this is to point at empheral storage when using k8s
datajobs.temp.storage.folder=${DATAJOBS_TEMP_STORAGE_FOLDER:}


# List of file types that are allowed to be uploaded.
# It is comma separated list with file types. For example "image/png,text/plain"
# Only base type can be specified as well, then all files with that base type are allowed.
# For example to allow all text files say "text" only, then any file with type "text/xxx" will be allowed.
# Instead to allow only sql and ini text files specify "text/x-sql,text/x-ini"
# Full list of file types are documented in https://tika.apache.org
# If set to empty, then all file types are allowed.
upload.validation.fileTypes.allowlist=${UPLOAD_VALIDATION_FILETYPES_ALLOWLIST:}
# List of file extensions that are allowed to be uploaded. Comma separated list e.g: "py,csv,sql"
# only files with extensions that are present in this list will be allowed to be uploaded.
# if the list is empty all extensions are allowed.
upload.validation.fileExtensions.allowlist=${UPLOAD_VALIDATION_EXTENSIONS_ALLOWLIST:}

# List of file types that are automatically deleted from data job source code
# before upload. Expects a comma separated list of file endings and deletes all
# files that match it - e.g "pyc,exe" etc. Operation is executed before the allow list
# validation above. If the list is empty no files will be deleted.
upload.validation.fileTypes.filterlist=${UPLOAD_VALIDATION_FILETYPES_FILTER_LIST:}
# List of file extensions that are automatically deleted from data job source code before upload.
# Comma separated list e.g: "pyc,exe,sh". If the list is empty no files will be deleted.
# Files are first deleted before the allow list performs its checks.
upload.validation.fileExtensions.filterlist=${UPLOAD_VALIDATION_EXTENSIONS_FILTER_LIST:}

# If the job builder image is saved in a private docker registry then this
# property should have the name of the secret in the env.
datajobs.builder.registrySecret=${DATAJOBS_BUILDER_REGISTRY_SECRET:}

# AWS Credentials. Needed for authenticating to ECR via the aws-cli: https://aws.amazon.com/cli/
datajobs.aws.accessKeyId=${AWS_ACCESS_KEY_ID}
datajobs.aws.secretAccessKey=${AWS_ACCESS_KEY_SECRET}

# Variables for using the Service Account pattern. Must be set if the data job builder is to use
# an AWS ECR repository accessed by an IAM user instead of long term credentials.
# The pattern involves creating an AWS Identity and Access Management (IAM)
# user account with a unique access key and secret key.
# The application running on the EC2 instance can then use the access key and secret key
# to authenticate with AWS services and make API calls on behalf of the IAM user account.
# By using a separate IAM user, the principle of least privilege is enforced.
# https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
# The service user variables are different than the AWS credentials - datajobs.aws.accessKeyId
# and secretAccessKey used by the control service.
#
# Necessary steps to setup Service Account:
# - Create a Service Account in Amazon (regular account that will be granted permissions to ECR repo) e.g:
#   https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console
#   e.g - IAM user: svc.example-service-user
# - Create a role in your AWS account (account where your ECR repository lives) - https://docs.aws.amazon.com/cli/latest/reference/iam/create-role.html
#   e.g - IAMRole: svc.example-service-user, the role must have the same name as your service user.
#   More information on what the example-service-user document should include can be found in the above link
#   which describes the process of creating a role.
# - Create a trust relationship on the role to the control account - edit your newly created role's Trust Relationship:
#   This is the trust relationship in your main account which is associated with the service user. Or
#   the role ARN - which is used to delegate access to resources securely.
#   More information on how to create and use role ARN can be found in the below mentioned documents.
#   https://aws.amazon.com/blogs/security/how-to-use-external-id-when-granting-access-to-your-aws-resources/
#   https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html.
# - Verify access to the linked account:
#   aws sts get-caller-identity with the Service Account credentials - this is the account for which
#   a role was created in the main account (where the Elastic Container Repository is stored)
#   Any AWS account can be created for the purpose of being a service account.
# If everything is configured correctly you need to fill in the below values and ECR repository
# access through the control-service (when deploying data job images) will go through the service account.
# datajobs.aws.assumeIAMRole tells the control-service if the Service Account pattern should be used.
datajobs.aws.assumeIAMRole=${DATAJOBS_AWS_ASSUME_IAM_ROLE:false}
# All flags must be set if the datajobs.aws.assumeIAMRole flag is true
# datajobs.aws.serviceAccountAccessKeyId and serviceAccountSecretAccessKey are the IAM user specific credentials
datajobs.aws.serviceAccountAccessKeyId=${DATAJOBS_AWS_SERVICE_ACCOUNT_ACCESS_KEY_ID:}
datajobs.aws.serviceAccountSecretAccessKey=${DATAJOBS_AWS_SERVICE_ACCOUNT_SECRET_ACCESS_KEY:}
# datajobs.aws.RoleArn is the Role that the IAM user will assume. Note that this role should be
# created in the main AWS and proper privileges should be granted to the IAM users that will assume it.
datajobs.aws.roleArn=${DATAJOBS_AWS_ROLE_ARN:}
# datajobs.aws.defaultSessionDurationSeconds is the default credential expiration for a job builder
# instance. Default value is 30 minutes.
datajobs.aws.defaultSessionDurationSeconds=${DATAJOBS_AWS_DEFAULT_SESSION_DURATION_SECONDS:1800}

# Hashicorp Vault Integration settings
# When disabled/not configured the Secrets functionality won't work
# In production, we would only use AppRole Authentication which require setup in vault:
# https://developer.hashicorp.com/vault/docs/auth/approle
# https://developer.hashicorp.com/vault/tutorials/auth-methods/approle
# and should provide roleid and secretid to the service.
#
# For local development you can start a vault server with the following command:
# vault server -dev -dev-root-token-id="root"
# and configure only the uri and the token
featureflag.vault.integration.enabled=false
# If you get 404 errors related to vault operations, double-check the vault URI, it should usually end with "/v1/"
vdk.vault.uri=http://localhost:8200/v1/
vdk.vault.approle.roleid=
vdk.vault.approle.secretid=
vdk.vault.token=
vdk.vault.kvstore=secret
vdk.vault.kvstoremeta=secret/metadata/
datajobs.vault.size.limit.bytes=1048576

datajobs.jfrog.artifactory.url=${JFROG_ARTIFACTORY_URL:}
datajobs.jfrog.artifactory.username=${JFROG_ARTIFACTORY_USERNAME:}
datajobs.jfrog.artifactory.password=${JFROG_ARTIFACTORY_PASSWORD:}
datajobs.jfrog.artifactory.repo=${JFROG_ARTIFACTORY_REPO:}
