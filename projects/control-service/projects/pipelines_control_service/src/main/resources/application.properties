# The file contains default configuration used in both production and development setup.

# Used for log file name
taurus.svc.name=data-jobs
# http://localhost:8080/${svc.url.prefix}/debug/servertime
sccp.svc.url.prefix=data-jobs
# http://localhost:8080/${svc.url.prefix}/debug
management.endpoints.web.base-path=/data-jobs/debug
management.endpoints.web.exposure.include=*

datajobs.version=0.0.1
springdoc.swagger-ui.path=/${taurus.svc.name}/swagger-ui.html
springdoc.api-docs.path=/${taurus.svc.name}/api-docs
spring.profiles.active=dev

# When updating or adding ports for services, these changes must be made in many places:
# 1) In each of the values.yml files at /k8s/envs/*/values.yml
# 2) In each service's application.properties at /services/*/src/main/resources/application.*
# 3) In base's service discovery enum at /libraries/base/src/main/java/com/vmware/taurus/discovery/ServiceLocation.java
server.port=8092

# We use Flyway (https://flywaydb.org/) for explicit database schema definition and migrations, so we disable Hibernate
# automated schema management. The automated management would be too opaque.
# The explicit migrations are defined in /src/main/resources/db.migration.
spring.jpa.hibernate.ddl-auto=none

# show-sql doesn't use logging but just prints to stdout
# to see SQL statements via logging enable DEBUG level on logger org.hibernate.SQL
# to see parameter values of SQL statements enable TRACE level on logger org.hibernate.type.descriptor.sql.BasicBinder
# NB: you can use a spring or JVM property in the form - logging.level.LOGGER_NAME=LEVEL - to override a level
# or environment variable LOGGING_LEVEL_LOGGER_NAME=LEVEL
# See https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-custom-log-levels
spring.jpa.show-sql=false
spring.jpa.properties.hibernate.show_sql=false
logging.level.org.hibernate.SQL=INFO
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=INFO
logging.level.com.vmware.taurus.service.locks.CustomLockProvider=INFO


spring.jpa.open-in-view=false


# Security configuration. Note if you enable security, you may
# also enable authentication provider(s) and authorization
# featureflag.security.enabled=true

# Settings for Kerberos authentication provider
datajobs.security.kerberos.enabled=${SECURITY_KERBEROS_ENABLED:false}
# Properties mandatory if datajobs.security.kerberos.enabled=true
datajobs.security.kerberos.kerberosPrincipal=${SECURITY_KERBEROS_SERVICE_PRINCIPAL:""}
datajobs.security.kerberos.keytabFileLocation=${SECURITY_KERBEROS_SERVICE_KEYTAB_FILE_LOCATION:}
datajobs.security.kerberos.krb5ConfigLocation=${SECURITY_KERBEROS_KRB5_CONFIG_LOCATION:}

# Settings for OAuth2 authentication provider
datajobs.security.oauth2.enabled=true

# The JSON Web Key Set (JWKS) is a set of keys which contains the public keys
# used to verify any JSON Web Token (JWT) issued by the authorization server
# It is required.
spring.security.oauth2.resourceserver.jwt.jwk-set-uri=${SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK_SET_URI:http://localhost}
# spring.security.oauth2.resourceserver.jwt.issuer-uri=${SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUER_URI}

# Authorization configuration. Note if you enable authorization you should
# also point the webhook endpoint against which authorization is delegated
# in order for the feature to fully work
featureflag.authorization.enabled=false
datajobs.authorization.webhook.endpoint=
datajobs.authorization.jwt.claim.username=username


# Data Jobs post webhook settings (Create and Delete)
datajobs.post.create.webhook.endpoint=
datajobs.post.create.webhook.internal.errors.retries=3
datajobs.post.delete.webhook.endpoint=
datajobs.post.delete.webhook.internal.errors.retries=3

# The owner name and email address that will be used to send all Versatile Data Kit related email notifications.
datajobs.notification.owner.email=versatiledatakit@groups.vmware.com
datajobs.notification.owner.name=Versatile Data Kit
# datajobs.notification.cc.emails=cc1@x.com,cc2@x.com

# The gitlab repository and credentials for pulling data jobs code when building their images.
datajobs.git.url=${GIT_URL}
datajobs.git.username=${GIT_USERNAME}
datajobs.git.password=${GIT_PASSWORD}
datajobs.git.branch=${GIT_BRANCH:master}
datajobs.git.remote=${GIT_REMOTE:origin}
datajobs.git.ssl.enabled=${GIT_SSL_ENABLED:true}
# The registry type, if left blank, defaults to ecr. The alternative registry type is generic
# which means the registry is of the Harbor or Dockerhub type.
datajobs.docker.registryType=generic

# Docker repository used to store data job images
datajobs.docker.repositoryUrl=ghcr.io/versatile-data-kit-dev/dp
datajobs.docker.registrySecret=${DOCKER_REGISTRY_SECRET:}

# Docker repository used to store VDK SDK image
datajobs.vdk.docker.registrySecret=${VDK_SDK_DOCKER_REGISTRY_SECRET:}

# open API generator is not generating example for parameters and causing noisy warnings
logging.level.io.swagger.models.parameters.AbstractSerializableParameter=ERROR


datajobs.control.k8s.kubeconfig=${HOME}/.kube/config
datajobs.control.k8s.k8sSupportsV1CronJob=false
datajobs.control.k8s.jobTTLAfterFinishedSeconds=3600
# Location to a K8s cronjob yaml file which will be used as a template
# for all data jobs. If the location is missing, the default internal
# cronjob yaml resource will be used (see k8s-data-job-template.yaml).
# Note that this location is expected to be set via the environment
# variable '# variable 'K8S_DATA_JOB_TEMPLATE_FILE'.'.
datajobs.control.k8s.data.job.template.file=${K8S_DATA_JOB_TEMPLATE_FILE:#{null}}

datajobs.monitoring.sync.interval=5000
datajobs.monitoring.sync.initial.delay=10000

# The status watch interval is the time period (expressed in milliseconds) after a status
# watch operation has completed and before a new one is started
datajobs.status.watch.interval=1000
# The status watch initial delay is the period (expressed in milliseconds) between control service
# start and the first time a data job status watch is started by the control service instance
datajobs.status.watch.initial.delay=10000


# The base image which will be used to create the image where data job would be run
# On top of it the job source and its dependencies are installed for each job
datajobs.deployment.dataJobBaseImage=python:3.9-slim

# The map of python version and respective data job base and vdk images that would be
# used for data job deployments
datajobs.deployment.supportedPythonVersions=${DATAJOBS_DEPLOYMENT_SUPPORTED_PYTHON_VERSIONS:{3.9: {vdkImage: 'registry.hub.docker.com/versatiledatakit/quickstart-vdk:release', baseImage: 'python:3.9-slim'}}}
# The default python version, which is to be used when selecting the vdk and job base images from
# the supportedPythonVersions for data job deployments
datajobs.deployment.defaultPythonVersion=${DATAJOBS_DEPLOYMENT_DEFAULT_PYTHON_VERSION:3.9}

#Configuration variables used for data job execution cleanup
#This is a spring cron expression, used to schedule the clean up job / default is every 3 hours
datajobs.executions.cleanupJob.scheduleCron=0 0 */3 * * *
#This variable is used to determine the maximum amount of executions the database can store / default is 100
#exposed in helm chart, if there are more executions than maximum, older ones will be deleted when clean up job runs.
datajobs.executions.cleanupJob.maximumExecutionsToStore=${DATAJOBS_EXECUTION_MAXIMUM_EXECUTIONS_TO_STORE:100}
#This variable exposes the total time to live of data job execution in seconds / default is 14 days
#executions older than that will get deleted when the clean up job runs
datajobs.executions.cleanupJob.executionsTtlSeconds=${DATAJOBS_EXECUTION_TTL_SECONDS:1209600}

# This template will be used for building of logs URL for each data job execution returned by API.
# Supported variables which will be replaced in the template with the particular execution values:
# {{execution_id}}, {{job_name}}, {{op_id}}, {{start_time}} and {{end_time}}
# Example: "https://log-insight-url/li/query/stream?query=%C2%A7%C2%A7%C2%A7AND%C2%A7%C2%A7%C2%A7%C2%
# A7{{start_time}}%C2%A7{{end_time}}%C2%A7true%C2%A7COUNT%C2%A7text:CONTAINS:{{execution_id}}*"
datajobs.executions.logsUrl.template=${DATAJOBS_EXECUTIONS_LOGS_URL_DATE_TEMPLATE:}

# This variable is used to determine the format of execution.startTime
# and execution.endTime used in the template (datajobs.executions.logsUrl.template).
# Supported options: iso (https://bg.wikipedia.org/wiki/ISO_8601) and unix (https://en.wikipedia.org/wiki/Unix_time).
# If left blank, defaults to unix.
# Examples: iso -> "2021-12-03T15:34:54.822098Z", unix -> "1638545973226".
datajobs.executions.logsUrl.dateFormat=${DATAJOBS_EXECUTIONS_LOGS_URL_DATE_FORMAT:unix}

# The offset (expressed in seconds) that will be added to or subtracted from the {{start_time}}
# variable during the building of logs URL. It could be either positive or negative number.
# In case of positive number it will be added to the {{start_time}}.
# In case of negative number it will be subtracted from the {{start_time}}.
# If left blank, defaults to 0.
datajobs.executions.logsUrl.startTimeOffsetSeconds=${DATAJOBS_EXECUTIONS_LOGS_URL_START_TIME_OFFSET_SECONDS:0}

# The offset (expressed in seconds) that will be added to or subtracted from the {{end_time}}
# variable during the building of logs URL. It could be either positive or negative number.
# In case of positive number it will be added to the {{end_time}}.
# In case of negative number it will be subtracted from the {{end_time}}.
# If left blank, defaults to 0.
datajobs.executions.logsUrl.endTimeOffsetSeconds=${DATAJOBS_EXECUTIONS_LOGS_URL_END_TIME_OFFSET_SECONDS:0}

# https://javaee.github.io/javamail/docs/api/com/sun/mail/smtp/package-summary.html
mail.transport.protocol= ${MAIL_TRANSPORT_PROTOCOL:smtp}
mail.smtp.host=${MAIL_SMTP_HOST:smtp.vmware.com}
# Fill these in if the notification emails need to go through an authenticated email.
mail.smtp.auth=${MAIL_SMTP_AUTH:false}
mail.smtp.starttls.enable=${MAIL_SMTP_STARTTLS_ENABLE:false}
mail.smtp.user=${MAIL_SMTP_USER:}
mail.smtp.password=${MAIL_SMTP_PASSWORD:}
mail.smtp.ssl.protocols= ${MAIL_SMTP_SSL_PROTOCOLS:TLSv1.2}
# Default port is 25 for unathenticated, 587 for authenticated
mail.smtp.port=${MAIL_SMTP_PORT:25}

# Set imagePullPolicy of the job-builder image
# Correspond to those defined by kubernetes
# See https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
datajobs.deployment.builder.imagePullPolicy=IfNotPresent

datajobs.deployment.builder.securitycontext.runAsUser=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_RUN_AS_USER:0}
datajobs.deployment.builder.securitycontext.runAsGroup=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_RUN_AS_GROUP:1000}
datajobs.deployment.builder.securitycontext.fsGroup=${DATAJOBS_DEPLOYMENT_BUILDER_SECURITY_CONTEXT_FS_GROUP:1000}

datajobs.deployment.builder.serviceAccountName=${DATAJOBS_DEPLOYMENT_BUILDER_SERVICE_ACCOUNT_NAME:}

datajobs.deployment.readOnlyRootFilesystem=${DATAJOBS_READ_ONLY_ROOT_FILESYSTEM:false}

datajobs.deployment.initContainer.resourreadOnlyRootFilesystem=${DATAJOBS_READ_ONLY_ROOT_FILESYSTEM:false}

# Resources set on Data Job initContainer
datajobs.deployment.initContainer.resources.requests.memory=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_REQUESTS_MEMORY:100Mi}
datajobs.deployment.initContainer.resources.requests.cpu=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_REQUESTS_CPU:100m}
datajobs.deployment.initContainer.resources.limits.memory=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_LIMITS_MEMORY:100Mi}
datajobs.deployment.initContainer.resources.limits.cpu=${DATAJOBS_DEPLOYMENT_INITCONTAINER_RESOURCES_LIMITS_CPU:100m}


# Used to specify in which folders data jobs should be stored temporality while uploading/deleting or deploying.
# if this is not specified if will use a temp folder as the location. It will be a sub folder of the folder specified by `java.io.tmpdir`.
# the main purpose of this is to point at empheral storage when using k8s
datajobs.temp.storage.folder=${DATAJOBS_TEMP_STORAGE_FOLDER:}


# List of file types that are allowed to be uploaded.
# It is comma separated list with file types. For example "image/png,text/plain"
# Only base type can be specified as well, then all files with that base type are allowed.
# For example to allow all text files say "text" only, then any file with type "text/xxx" will be allowed.
# Instead to allow only sql and ini text files specify "text/x-sql,text/x-ini"
# Full list of file types are documented in https://tika.apache.org
# If set to empty, then all file types are allowed.
upload.validation.fileTypes.allowlist=${UPLOAD_VALIDATION_FILETYPES_ALLOWLIST:}


# If the job builder image is saved in a private docker registry then this
# property should have the name of the secret in the env.
datajobs.builder.registrySecret=${DATAJOBS_BUILDER_REGISTRY_SECRET:}

# AWS Credentials. Needed for authenticating to ECR via the aws-cli: https://aws.amazon.com/cli/
datajobs.aws.accessKeyId=${AWS_ACCESS_KEY_ID}
datajobs.aws.secretAccessKey=${AWS_ACCESS_KEY_SECRET}

# Variables for using the Service Account pattern. Must be set if the data job builder is to use
# an AWS ECR repository accessed by an IAM user instead of long term credentials.
# The AWS Service Account pattern is a design pattern used to manage access to AWS resources.
# The pattern involves creating an AWS Identity and Access Management (IAM)
# user account with a unique access key and secret key. The access key and secret key are then
# stored in a secure location, such as an encrypted file, on the EC2 instance.
# The application running on the EC2 instance can then use the access key and secret key
# to authenticate with AWS services and make API calls on behalf of the IAM user account.
# By using a separate IAM user account for each application, the principle of least privilege is
# enforced and the attack surface is reduced.
# https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
# These need to be stored in different variables than the AWS credentials - datajobs.aws.accessKeyId
# and secretAccessKey used by the control service, because if the K8S cluster is hosted on a EKS
# instance, some API calls to the instance made by the control-service could fail.
# datajobs.aws.assumeIAMRole tells the control-service if the Service Account pattern should be used.
datajobs.aws.assumeIAMRole=${DATAJOBS_AWS_ASSUME_IAM_ROLE:false}
# All flags must be set if the datajobs.aws.assumeIAMRole flag is true
# datajobs.aws.serviceAccountAccessKeyId and serviceAccountSecretAccessKey are the IAM user specific credentials
datajobs.aws.serviceAccountAccessKeyId=${DATAJOBS_AWS_SERVICE_ACCOUNT_ACCESS_KEY_ID:}
datajobs.aws.serviceAccountSecretAccessKey=${DATAJOBS_AWS_SERVICE_ACCOUNT_SECRET_ACCESS_KEY:}
# datajobs.aws.RoleArn is the Role that the IAM user will assume. Note that this role should be
# created in the main AWS and proper privileges should be granted to the IAM users that will assume it.
datajobs.aws.roleArn=${DATAJOBS_AWS_ROLE_ARN:}
# datajobs.aws.defaultSessionDurationSeconds is the default credential expiration for a job builder
# instance. Default value is 30 minutes.
datajobs.aws.defaultSessionDurationSeconds=${DATAJOBS_AWS_DEFAULT_SESSION_DURATION_SECONDS:1800}

# Hashicorp Vault Integration settings
# When disabled/not configured the Secrets functionality won't work
featureflag.vault.integration.enabled=false
vdk.vault.uri=http://localhost:8200
vdk.vault.token=root
