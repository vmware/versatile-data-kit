# Copyright 2021-2023 VMware, Inc.
# SPDX-License-Identifier: Apache-2.0

## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameter(s): imageRegistry
## Set global.imageRegistry if you use docker proxy to the proxy prefix for versatiledatakit.
##
global: {}
#   imageRegistry: myRegistryName
#     - myRegistryKeySecretName

# Image configuration of the Control service image. Generally those should be left as it is.
image:
  registry: registry.hub.docker.com/versatiledatakit
  repository: pipelines-control-service
  tag: "1f57f0e"
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  registryUsernameReadOnly:
  registryPasswordReadOnly:

# The image configuration for builder jobs used during deployment of data jobs.
# Generally those should be left as it is.
deploymentBuilderImage:
  registry: registry.hub.docker.com/versatiledatakit
  repository: job-builder
  tag: "1.2.3"
  username:
  password:

deploymentBuilder:
  securityContext:
    runAsUser: "0"
    runAsGroup: "1000"
    fsGroup: "1000"
  ## The name of the ServiceAccount to use.
  # serviceAccountName: ""


## String to partially override pipelines-control-service.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override pipelines-control-service.fullname template
##
# fullnameOverride:

## Service configuration
service:
   internalPort: 8092
   type: LoadBalancer # NodePort
   externalTrafficPolicy: Cluster
   # nodePort:
   # loadBalancerSourceRanges:
   # loadBalancerIP:
   # clusterIP
   # additionalLabels:

### Ingress resource parameters
ingress:
  ## Configure ingress resource to expose the service outside of K8s. The ingress resource will serve as reverse proxy,
  ## load balancer, terminate SSL connections and support SNI to route HTTP traffic to the local service.
  ## F.e. it could aid with the following scenario:
  ##   - HTTP/HTTPS client redirects
  ##   - Rewrite rules: rewrite URI based on regex match (see annotations document for examples)
  ##   - Single entrypoint to terminate SSL connections (ingress.tls_secret is required to support HTTPS)
  ##
  ## More in-depth documentation is available here:
  ##   https://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ## Ingress controller should already be available on the cluster. Unlike some other controllers, it is deployed
  ## separately. Chances are there is one already present.
  enabled: false
  ## Add custom annotations to the ingress resource. Objects below will update the ingress runtime configuration.
  ## Supported annotations: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/
  annotations: {}
  # kubernetes.io/ingress.class: nginx
  ## Configure hostname to use in named-based virtual host.
  host:
  ## URI path/endpoint to the service
  ## This should be unique path to instruct the ingress controller which path is served by the downstream service.
  ## Defaults to '/'
  ## This allows the service to co-exist and share the same hostname with other services.
  path: ""
  ## Provide existing secret which holds TLS certificate/key.
  ## This is will instruct the ingress controller to use specific TLS certificate on this ingress resource. By default,
  ## we use community Nginx controller which supports SNI, thus a signle IP could terminate all connections.
  ## https://github.com/kubernetes/ingress-nginx/
  tls_secret: ""

# Embedded server settings
server:
   ## Max Http Header Size - defaults to 8KB
   maxHttpHeaderSize: 8KB

credentials:
   ## Configuration for automatic management of Data Job Credentials and Users for connecting to data plane (databases usually).
   ## Then they can managed by the SRE operation team which they can do credential rotations, invalidation, etc.
   ## If enabled, each data job is automatically provisioned principal (users) and credentials (e.g passwords, keys).
   ## And upon Cloud execution they are injected into the data job and picked up by the SDK.
   ## THe following types are possible
   ## * EMPTY - no principal nor credentials are automatically provisioned. Thus effectively disables the feature. User needs to manually provision and configure credentials as job secrets.
   ## * KERBEROS - when set kerberos user and keytab file are created for each data job. kerberosXXX options below are required then.
   repository: "EMPTY"

   ## Credentials principal pattern. It must have "%(data_job)" in the pattern which will be replaced by the name of the data job.
   ## Different patterns can be given to different data-pipelines deployments that share the same authentication repository (for example same Kerberos KDC).
   ## This way they won't interfere with each other.
   ## Generally should be left as default. It should be used only if multiple deployments of Versatile Data Kit that use same authentication repository.
   ## The default principal name in VDK must be the same as this one.
   ## Or when running locally it needs to overridden (environmental variable VDK_KEYTAB_PRINCIPAL)
   ## During cloud runs that variable is set automatically.
   principalPattern: "pa__view_%(data_job)"

   ## [Required if credentials.repository is "KERBEROS" otherwise ignored]
   ## Used to create automatically users in KDC server for each data job.
   ## If left empty no users will be created for each data job. # TODO: currently operations fail though ...
   ## https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html
   kerberosKrb5Conf: ""
   ## Kerberos KDC admin server configuration and credentials
   kerberosKadminUser: ""
   kerberosKadminPassword: ""

## The owner name and email address that will be used to send all Versatile Data Kit related email notifications.
notificationOwnerEmail: "taurus@vmware.com"
notificationOwnerName: "Versatile Data Kit"
### Coma separate list of mails which will be cc-ed for Control Service managed notifications
### This does not apply for notifications managed by external services (like AlertManager)
#notificationCcEmails: "a1@x.com,a2@x.com"


### deploymentXXX refer to properties used in order to deploy a Data Job.

## This is the kubernetes namespace where the Data Jobs will be deployed.
## If left empty it will use the same cluster as the Control Plane which is not recommended for production setup.
deploymentK8sNamespace: ""
## If you have data jobs are to be deployed into different cluster . Generally used during testing.
deploymentK8sKubeconfig: ""

## This is the kubernetes namespace builder and other system jobs used by Control Service are run
## It's is ok to leave it empty (by default).
controlK8sNamespace: ""

## [Required] Git repository where deployed data jobs source can be found and from where they will be deployed.
# Git URL (without https:// scheme)
deploymentGitUrl: ""
deploymentGitBranch: "main"
deploymentGitRemote: "origin"
# Credentials with read-only access to the Git repository.
deploymentGitUsername: ""
deploymentGitPassword: ""
# Credentials with read and write access to the Git repository.
uploadGitReadWriteUsername: ""
uploadGitReadWritePassword: ""

# List of file types that are allowed to be uploaded.
# It is comma separated list with file types. For example "image/png,text/plain"
# Only base type can be specified as well, then all files with that base type are allowed.
# For example to allow all text files say "text" only, then any file with type "text/xxx" will be allowed.
# Instead to allow only sql and ini text files specify "text/x-sql,text/x-ini"
# Full list of file types are documented in https://tika.apache.org
# If set to empty, then all file types are allowed.
uploadValidationFileTypesAllowList: ""

## [Required] The repository where the data job images will be stored (so write permission must be granted)
## Automatically a repository will be created for each data job in ${deploymentDockerRepository}/data-job-name
## (without https:// scheme)
deploymentDockerRepository: "012285273210.dkr.ecr.us-west-2.amazonaws.com/taurus/dp"
## [Required] Public facing proxy used to when pulling data job docker images. This can be configured the same as
## deploymentDockerRepository variable below in case publicly accessible repository like Dockerhub is used
## It will be used to pull images from Data Job repos created - ${proxyRepositoryURL}/data-job-name
## The repositories should not require credentials (for creating public ECR repos see https://github.com/monken/aws-ecr-public)
## or the Kubernetes cluster and service account should be configured with appropriate authentication (set imagePullSecrets)
## (without https:// scheme)
proxyRepositoryURL: "012285273210.dkr.ecr.us-west-2.amazonaws.com/taurus/dp"

## Image registry types. Your account need to have pull and push permissions over the registry. The supported values are:
## - "ecr" - AWS ECR Registry (https://aws.amazon.com/ecr) - in this case you need to set deploymentEcr* variables above
## - "generic" : Dockerhub or Harbor type registries - in this case you need to set deploymentDockerRegistry* variables below
## If left blank defaults to ecr registry type.
deploymentDockerRegistryType: "ecr"

## [Required if deploymentDockerRegistryType=ecr]
## These values also set the default Amazon ENV variables by convention.
## AWS ECR Credentials and repository to store and read images of the deployed data jobs
## The configuration should be set in case deploymentDockerRegistryType is ecr
deploymentEcrAwsRegion: ""
deploymentEcrAwsAccessKeyId: ""
deploymentEcrAwsAccessKeySecret: ""

##
deploymentEcrAwsAssumeIamRole: false

## [Required if deploymentEcrAwsAssumeIamRole=true and deploymentDockerRegistryType=ecr]
## Values that will enable the service pattern to be used by the control-service.
## The pattern involves creating an AWS Identity and Access Management (IAM)
## user account with a unique access key and secret key. The access key and secret key are then
## stored in a secure location, such as an encrypted file, on the EC2 instance.
## The IAM user is then given specific permissions to only access the ECR registry - read/write images
## and create repositories.
## More information: # https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
## These properties need to be stored in different env variables than the ones not belonging to the
## service user - e.g. defined above, because if the data jobs K8S cluster is hosted on a EKS
## instance, some API calls to the instance made by the control-service could fail since we set
## the default AWS environment variable names above which are used by default on some k8s api calls.
deploymentEcrAwsServiceAccountAccessKeyId: ""
deploymentEcrAwsServiceAccountSecretAccessKey: ""
deploymentEcrAwsRoleArn: ""
deploymentEcrAwsDefaultSessionDurationSeconds: 1800

## [Required if deploymentDockerRegistryType=generic and registry requires authentication]
## Username and Password credentials in case the deploymentDockerRegistryType is generic.
## Dockerhub account credentials or credentials of the robot account in a Harbor Project.
## The account with those credentials needs to have push and pull permissions over the registry.
deploymentDockerRegistryUsername: ""
deploymentDockerRegistryPassword: ""

## [Required if deploymentDockerRegistryType=generic and registry requires authentication]
## Username and Password credentials in case the deploymentDockerRegistryType is generic.
## Container registry credentials used for authenticating when pulling Data Job images.
## The account with those credentials needs to have pull permissions over the registry.
deploymentDockerRegistryUsernameReadOnly: ""
deploymentDockerRegistryPasswordReadOnly: ""

## Image name of VDK which will be used to run the data jobs.
## It is recommended to use image with same tag (e.g release or latest)
## As it will be pulled before any execution thus making sure all jobs use the same and latest version of VDK.
## The image should contain installation of VDK (vdk) which will be used to run the data jobs (vdk run command)
## Only the installed python modules (vdk and its dependencies) will be used from the image.
## Everything else is effectively discarded since another image is used as base during execution.
## Override if you build VDK SDK yourself instead of using one provided by default.
deploymentVdkDistributionImage:
  registry: registry.hub.docker.com/versatiledatakit

  ## Image registry types. Your account need to have pull permission over the registry. The supported values are:
  ## - "generic" : Dockerhub or Harbor type registries - in this case you need to set deploymentVdkDistributionImage.registry* variables below
  registryType: ""

  ## [Required if deploymentVdkDistributionImage.registryType=generic and registry requires authentication]
  ## Username and Password credentials in case the deploymentVdkDistributionImage.registryType is generic.
  ## Container registry credentials used for authenticating when pulling VDK SDK image.
  ## The account with those credentials needs to have pull permissions over the registry.
  registryUsernameReadOnly: ""
  registryPasswordReadOnly: ""

  repository: quickstart-vdk
  tag: "release"


# The base image which will be used to create the image where data job would be run.
# On top of it the job source and its dependencies are installed for each job.
# It can be customized further by installing other native dependencies necessary to run different kinds of libraries
# For example the below default image adds support for Oracle client on top of standard python image.
# The SDK (vdk) is installed separate image (to enable zero time upgrades) see deploymentVdkDistributionImage
deploymentDataJobBaseImage:
  registry: registry.hub.docker.com/versatiledatakit
  repository: data-job-base-python-3.7
  tag: "latest"
  username:
  password:

## ini formatted options that provides default and per-job VDK options.
## Allows to provide VDK configuration via environment variables.
## They will be set as environment variables during execution of Data Job
## Enables administrators to apply common configuration for all Data Jobs
## you may use helm --set-file file.ini
## See "example-vdk-options.ini"
vdkOptions: ""

## Resources set on all data jobs by default if not overridden per job
deploymentDefaultDataJobsResources:
   limits:
      cpu: 500m
      memory: 1G
   requests:
      cpu: 500m
      memory: 1G

deploymentBuilderResourcesDefault:
   limits:
      cpu: 1000m
      memory: 1000Mi
   requests:
      cpu: 250m
      memory: 250Mi

# deploymentAdditionalLabels:

### WebHooks configuration

webHooks:
   ## Webhooks that will delegate postCreate and postDelete actions to the devops team operating the Versatile Data Kit service.
   ## It can be configured during installation and whenever postCreate and postDelete actions need to be taken Versatile Data Kit API will call it
   ##
   ## HTTP POST request to <webhook address>/<requested_http_path> with body JSON content:
   ##
   ## * requester_user_id - this is retrieved from the Oauth2 token claims (see jwtClaimUsername configuration)
   ## * requested_resource_team - team that the job belongs to.
   ## * requested_resource_name - this is always "data-job"
   ## * requested_resource_id - the data job name
   ## * requested_http_path - The HTTP Path of the REST API request being invoked.
   ##                         For Create Job: it will be /data-jobs/for-team/{team-name}/jobs.
   ##                         For Delete Job: it will be /data-jobs/for-team/{team-name}/jobs/{job-name}.
   ##                         Example: /data-jobs/for-team/taurus/jobs/example.
   ## * requested_http_verb - The HTTP verb of the REST API request being invoked
   ##                         For Create Job: it will be POST.
   ##                         For Delete Job: it will be DELETE.
   ## Complete example:
   ### Post Delete WebHook:
   ### HTTP POST: https://webhook.com/data-jobs/for-team/taurus/jobs/example
   ### {
   ###    requester_user_id: auserov,
   ###    requested_resource_team: taurus,
   ###    requested_resource_name: data-job,
   ###    requested_resource_id: example,
   ###    requested_http_path: /data-jobs/for-team/taurus/jobs/example,
   ###    requested_http_verb: DELETE
   ###  }
   ###
   ### Post Create WebHook:
   ### HTTP POST: https://webhook.com/data-jobs/for-team/taurus/jobs
   ### {
   ###    requester_user_id: auserov,
   ###    requested_resource_team: taurus,
   ###    requested_resource_name: data-job,
   ###    requested_resource_id: example,
   ###    requested_http_path: /data-jobs/for-team/taurus/jobs,
   ###    requested_http_verb: POST
   ###  }
   ###
   ## Expected return results(for both the request types):
   ##    2xx means the web Hook Operation is successful and the job is created/deleted
   ##    4xx or 5xx the job is not created/deleted
   ## 4xx response content is shown to end user as an error message.
   ## 5xx responses will be retried (number of retries is based on configuration, defaults to 3)
   postCreate:
      webhookUri: ""
      internalErrorsRetries: 3
   postDelete:
      webhookUri: ""
      internalErrorsRetries: 3

### Security configuration

security:
   ## Feature flag to enable/disable security entirely
   enabled: false
   oauth2:
      ## [ Required if security.enabled = True ]
      ## The JSON Web Key Set (JWKS) is a set of keys which contains the public keys
      ## used to verify any JSON Web Token (JWT) issued by the authorization server
      ## Required if security is enabled
      ## See https://tools.ietf.org/html/rfc7517
      ## For example :
      ##    https://www.googleapis.com/oauth2/v3/certs
      ##    https://console.cloud.vmware.com/csp/gateway/am/api/auth/token-public-key?format=jwks
      ## JWT access token need be passed on requests in 'Authorization: Bearer' header
      jwtJwkSetUri: ""

      ## [ Required if security.enabled = True ]
      jwtIssuerUrl: ""
   kerberos:
      ## These values contain configuration settings for the kerberos authentication provider in control-service.
      ## The values are exposed as environment variables through the helm chart
      ## which get picked up by our spring application (contorl-service)
      ## on startup. More information about kerberos and spring can be found here:
      ## https://docs.spring.io/spring-security-kerberos/docs/current/reference/htmlsingle/index.html
      ## More information on kerberos authentication can be found here:
      ## https://docs.oracle.com/cd/E23941_01/E26092/html/kerberos-auth.html
      ## You only need to set the flag and property contents with correct values and kerberos authentication should work,
      ## provided global security is enabled.
      ##
      ## The krb5.conf file that we use is the one specified in the credentials section and is required for kerberos
      ## authentication to work. If kerberos authentication is enabled make sure the kerberosKrb5Conf property is set in
      ## the credentials section as described in the documentation.
      ##
      ## Flag to determine whether kerberos authentication should be enabled/disabled
      enabled: false
      ## The service principal of the application, for example
      ## "HTTP/full-qualified-domain-name@DOMAIN".
      ## For more information on kerberos principal please review the official kerberos documentation:
      ## https://web.mit.edu/kerberos/krb5-1.5/krb5-1.5.4/doc/krb5-user/What-is-a-Kerberos-Principal_003f.html
      kerberosServicePrincipal: ""
      ## [ Required if kerberos.enabled = True ]
      ## The keytab must contain the key for this principal.
      ## For more information on kerberos keytabs please review the official kerberos documentation:
      ## https://web.mit.edu/kerberos/krb5-devel/doc/basic/keytab_def.html
      ## You may use e.g. helm --set-file <filename.keytab> to set the contents of the file.
      kerberosServiceKeytab: ""

   ## Feature flag to enable/disable authorization
   authorizationEnabled: false
   authorization:
      ## [ Required if security.authorizationEnabled = True ]
      ## Webhook that will delegate authorization decisions.
      ## It can be configured during   installation and whenever authorization decision needs to be taken Versatile Data Kit API will call it
      ##
      ## HTTP POST request to webhook address with body JSON content:
      ##
      ## * requester_user_id - this is retrieved from the Oauth2 token claims (see jwtClaimUsername configuration)
      ## * requested_resource_team - team that the job belongs to configured during its creation or passed to create job request. Otherwise if job does not exists, it's empty.
      ## * requested_resource_name - this is always "data-job"
      ## * requested_resource_id - the data job name
      ## * requested_permission - it is "read" for operations that are READ only (e.g. HTTP GET) - "write" if modification is done
      ## * requested_http_path - The HTTP Path of the REST API request being authorized
      ## * requested_http_verb - The HTTP verb of the REST API request being authorized
      ##
      ## For example:
      ## {
      ##    requester_user_id: auserov,
      ##    requested_resource_team: job-team,
      ##    requested_resource_name: data-job,
      ##    requested_resource_id: example,
      ##    requested_permission: write
      ##    requested_http_path: /data-job/example/deployments,
      ##    requested_http_verb: POST
      ## }
      ## Expected return results:
      ##    2xx means access is granted,
      ##    4xx or 5xx access is rejected.
      ## 4xx response content is shown to end user as an error message.
      ## 5xx responses will be retried
      webhookUri: ""
      ## What JWT token claim (aka attribute/field) will fetch the username from.
      jwtClaimUsername: "username"

      ## The below four properties are used to authorized requests based on claim values (e.g. organization ids)
      ## and, optionally, based on roles. This type of authorization works in conjunction with the webhook-based
      ## one so if both are configured, both will take effect: first the claim values and roles, then the webhook.
      ##
      ## Name of a claim in the JWT token that needs to be validated.
      ## An example use case would be to validate if an organization id is allowed to access and modify resources.
      customClaimName: "context_name"
      ## A comma separated list of allowed values (e.g. list of authorized organization ids) for the JWT claim
      ## specified in "security.authorization.customClaimName". If the list is empty, the claim is not validated.
      authorizedCustomClaimValues:
      ## The Auth Token doesn't contain the user permissions in the default location - the authorities
      ## field of the JWT token. This property specifies a custom location for the authorities field.
      authoritiesClaimName: "perms"
      ## A comma separated list of roles. Users whose tokens contain at least one of the roles
      ## are allowed to access and modify resources. If the list is empty, roles are not validated.
      ## E.g. "f399c40d-fb5e-4d24-8041-20b82471c6be/taurus:service-owner,65ecda65-2975-4a9c-b982-ccf52755296d/tpcs:admin"
      authorizedRoles:

## TODO: add support for OpenIdConnect ID_TOKEN - Contact Taurus team if necessary

## Number of replicas of the Control Plane Deployment
replicas: 2
## Resources set on Control Plane Deployment
resources:
   limits:
      cpu: 500m
      memory: 500Mi
   requests:
      cpu: 500m
      memory: 500Mi

## [Required] Database configuration used by Control Plane
## Database is used to store Data Jobs metadata like name, team, description
## and some deploy specific configuration like execution schedule
## Database schema is internal implementation detail and should not be queried directly.

## Configuration for a PostgreSQL embedded database, set postgresql.enabled to false if you want to use external database
## See https://github.com/bitnami/charts/tree/main/bitnami/postgresql
postgresql:
  enabled: false
  architecture: standalone  # replication
#  readReplicas:
#    replicaCount: 2
#    resources:
#      limits:
#        cpu: 500m
#        memory: 500Mi
#      requests:
#        cpu: 500m
#        memory: 500Mi
  primary:
    persistence:
      size: 1Gi
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 500m
        memory: 500Mi
    service:
      type: ClusterIP

## Configuration for a CockroachDB embedded database, set cockroachdb.enabled to false if you want to use external database
## See https://github.com/cockroachdb/helm-charts/tree/master/cockroachdb
cockroachdb:
   enabled: true
   storage:
      persistentVolume:
         size: 1Gi
   statefulset:
      replicas: 3
      resources:
         limits:
            cpu: 500m
            memory: 500Mi
         requests:
            cpu: 500m
            memory: 500Mi
   init:
      resources:
         limits:
            cpu: 500m
            memory: 500Mi
         requests:
            cpu: 500m
            memory: 500Mi
   service:
      public:
         type: ClusterIP

## Configuration for external CockroachDB database
##
## If cockroachdb.enabled is true - it is ignored and embedded database is used (using default credentials)
## If embedded cockroach db is disabled either provide existing secret (database.secretName) or specify credentials.
## If both are set, externalSecretName will be used.
database:
   ## Name of the secret which holds JDBC credentials. The chart will not attempt to create this, but will use it as is.
   ## The secret should contain keys: JDBC, USERNAME, PASSWORD
   externalSecretName: ""
   ## Alternatively provide JDBC url, username and password. externalSecretName takes precendence if both are set.
   jdbcUrl: ""
   username: ""
   password: ""
## Prometheus exporter configuration
##
metrics:
   enabled: true
   ## Prometheus Operator ServiceMonitor configuration
   ##
   serviceMonitor:
      enabled: false
      ## Namespace in which Prometheus is running
      ##
      # namespace: monitoring

      ## Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: 30s

      ## Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: 10s

      ## Other options also can be set
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      honorLabels: false
      scheme: http
      # params:

      ## Custom labels added to the ServiceMonitor usually necessary by Prometheus to find the ServiceMonitor
      # selector

      ## Drop the execution_id label of the taurus_datajob_termination_status metrics to reduce its cardinality.
      ## Currently it is not being used by the Prometheus rules below.
      metricRelabelings:
         -  regex: 'execution_id'
            action: labeldrop

## Alerting configuration
##
alerting:
   enabled: false

   ## Custom PrometheusRule to be defined
   ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
   ##
   prometheusRule:
      additionalLabels: { }
      ## Specify the namespace where Prometheus Operator is running
      ##
      # namespace: monitoring

      ## Define individual alerting rules as required
      ## ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
      ##
      ## Below are some built-in rules that can be used as examples.
      ## The rules rely on the taurus_datajob_termination_status and taurus_datajob_info metrics (ref: README.md)
      ## as well as the kube_cronjob_next_schedule_time, kube_cronjob_info, and kube_job_status_completion_time
      ## metrics exposed by kube_state_metrics (ref: https://github.com/kubernetes/kube-state-metrics).
      rules:
         JobSuccess:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: >-
                  {{`{{ $labels.email_notified_on_success | reReplaceAll ";" "," }}`}}
               subject: >-
                  {{`[success][data job run] {{ $labels.data_job }}`}}
               content: >-
                  {{`Dear Versatile Data Kit user,<br>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> succeeded.
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: >-
               {{`label_replace(
                        (avg by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_success)
                        avg by(data_job, email_notified_on_success) (taurus_datajob_info{email_notified_on_success!=""}) == bool 0)
                    * on(data_job) group_left(job_name, namespace)
                      topk by(data_job) (1, label_replace(kube_job_status_completion_time, "data_job", "$1", "job_name", "(.*)-.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)")`}}
         JobDelay:
            enabled: true
            for: 5m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: >-
                  {{`{{ $labels.email_notified_on_platform_error | reReplaceAll ";" "," }}`}}
               subject: >-
                  {{`[data job delay] {{ $labels.data_job }}`}}
               content: >-
                  {{`Dear Versatile Data Kit user,
                  <p>
                  Your data job <strong>{{ $labels.data_job }}</strong> was delayed for more than {{ $value | printf "-(%f) / 60" | query | first | value | printf "%.f" }} minutes.
                  </p>
                  <p>
                  WHAT HAPPENED: The Data Job has been delayed from its expected execution schedule.<br>
                  WHY IT HAPPENED: Possible reason for this is that the Data Job execution is taking so long that the next scheduled execution could not start in time. Or, there could be a platform error preventing the Data Job execution from starting.<br>
                  CONSEQUENCES: Your Data Job execution did not start in time, or, potentially, did not start at all.<br>
                  COUNTERMEASURES: Please review the executions of your latest data job runs and if they take longer than expected, increase the scheduled interval by modifying the schedule_cron property or increase the notification_delay_period_minutes property in the config.ini file and redeploy the Data Job.
                  </p>
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            ## Add an "execution_id" label with value equal to data_job to allow JobDelay alerts
            ## to not be grouped together by Alertmanager
            expr: >-
               {{`label_replace(
                  label_replace(
                    (kube_cronjob_next_schedule_time > 0) - time(),
                    "data_job", "$1", "cronjob", "(.+)")
                  * on(data_job) group_left(email_notified_on_platform_error)
                    avg by(data_job, email_notified_on_platform_error) (taurus_datajob_info{email_notified_on_platform_error!=""})
               <
                  on(data_job)
                  group_right(email_notified_on_platform_error, namespace)
                  -(min by(data_job) (taurus_datajob_notification_delay) * 60),
               "execution_id", "$1", "data_job", "(.*)")`}}
         JobFailurePlatform:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: >-
                  {{`{{ $labels.email_notified_on_platform_error | reReplaceAll ";" "," }}`}}
               subject: >-
                  {{`[platform error][data job run] {{ $labels.data_job }}`}}
               content: >-
                  {{`Dear Versatile Data Kit user,
                  <p>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> has failed.
                  </p>
                  <p>
                  There has been a platform error. Here are the details:<br>
                  WHAT HAPPENED: The last execution of your Data Job has failed.<br>
                  WHY IT HAPPENED: There has been a platform error. The Data Job has been restarted automatically up to 3 times but all of the attempts failed.<br>
                  CONSEQUENCES: Your Data Job execution did not complete successfully.<br>
                  COUNTERMEASURES: No customer action is required. If you want to receive notifications on successful executions or you don't want to receive notifications on execution failures, you can set it up in the config.ini file and redeploy the Data Job.
                  </p>
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: >-
               {{`label_replace(
                  label_replace(
                        (max by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_platform_error)
                        avg by(data_job, email_notified_on_platform_error) (taurus_datajob_info{email_notified_on_platform_error!=""}) == bool 1)
                    * on(data_job) group_left(job_name, namespace)
                      topk by(data_job) (1, label_replace(kube_job_failed * on(job_name) group_left() kube_job_status_start_time, "data_job", "$1", "job_name", "(.*)-.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)"),
                    "short_execution_id", "$1", "execution_id", "([a-zA-Z -_]{1,58}).*")`}}
         JobFailureUser:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: >-
                  {{`{{ $labels.email_notified_on_user_error | reReplaceAll ";" "," }}`}}
               subject: >-
                  {{`[user error][data job run] {{ $labels.data_job }}`}}
               content: >-
                  {{`Dear Versatile Data Kit user,
                  <p>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> has failed.
                  </p>
                  <p>
                  An error in the Data Job code occurred. The error should be resolved by data job owner. Here are the details:<br>
                  WHAT HAPPENED: The last execution of your Data Job has failed.<br>
                  WHY IT HAPPENED: The reasons for the failure could be: the job run out of memory, the job run for too long and was terminated, a query references tables or columns that do not exist, there is a logical error in the code, etc.<br>
                  CONSEQUENCES: Your Data Job execution did not complete successfully.<br>
                  COUNTERMEASURES: Inspect the logs of the Data Job execution and get clues about the error. Fix the problem and then redeploy the Data Job. If you want to receive notifications on successful executions or you don't want to receive notifications on execution failures, you can set it up in the config.ini file and redeploy the Data Job.
                  </p>
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: >-
               {{`label_replace(
                  label_replace(
                        (max by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_user_error)
                        avg by(data_job, email_notified_on_user_error) (taurus_datajob_info{email_notified_on_user_error!=""}) == bool 3)
                    * on(data_job) group_left(job_name, namespace)
                      topk by(data_job) (1, label_replace(kube_job_status_start_time, "data_job", "$1", "job_name", "(.*)-.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)"),
                    "short_execution_id", "$1", "execution_id", "([a-zA-Z -_]{1,58}).*")`}}

## Synchronization mechanism for consistent metrics. The time format is number in milliseconds
monitoringSync:
   ## Initial delay between service startup and first update of metrics
   initialDelayMillis: 10000
   ## Interval in which a single instance of the service will synchronize its metrics from centralized location
   ## Note: this should be lower than the prometheus scrape configuration so we make sure every service exposes
   ## latest monitoring metrics for prometheus to scrape
   delayMillis: 60000

## Role Based Access in Kubernetes
## ref: https://kubernetes.io/docs/admin/authorization/rbac/
## The Control service needs permission to create job.batch API resource.
## Generally those should be left as it is.
rbac:
   ## Specifies whether RBAC rules should be created
   ## binding Versatile Data Kit Control Service Kubernetes ServiceAccount to a role
   ## that allows Control Service pods querying the K8s API
   create: true
   ## Specifies whether RBAC rules should be created for Data Jobs deployment namespace (see deploymentK8sNamespace)
   ## If this is set to false - deploymentK8sKubeconfig must be set with required permissions (see role_datajobs.yaml)
   datajobsDeployment:
      create: true

## Versatile Data Kit Control Service pods ServiceAccount
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
   ## Specifies whether a ServiceAccount should be created
   ##
   create: true
   ## The name of the ServiceAccount to use.
   ## If not set and create is true, a name is generated using the pipelines-control-service.fullname template
   ##
   # name:


## This is deprecated ; use extraEnvVars
# extraVars:
# - name: EXTRA_VAR_1
#   value: extra-var-value-1
# - name: EXTRA_VAR_2
#   value: extra-var-value-2

## Pass extra environment variables to the Versatile Data Kit Control Service container.
#extraEnvVars:
#  EXTRA_VAR_1: extra-var-value-1
#  EXTRA_VAR_2: extra-var-value-2


## Configure extra options for liveness and readiness and startup probes
## Control Services exposes  /data-jobs/debug/health to unauthenticated requests, making it a good
## default liveness and readiness path. However, that may not always be the
## case. For example, if the image value is overridden to an image containing a
## module that alters that route.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
##
livenessProbe:
   enabled: true
   path: /data-jobs/debug/health/liveness
   initialDelaySeconds: 120
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 4
   successThreshold: 1
readinessProbe:
   enabled: true
   path: /data-jobs/debug/health/readiness
   initialDelaySeconds: 30
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 4
   successThreshold: 1
startupProbe:
   enabled: true
   path: /data-jobs/debug/health/liveness
   initialDelaySeconds: 30
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 30
   successThreshold: 1

## Custom Liveness probe
##
customLivenessProbe: {}

## Custom Readiness probe
##
customReadinessProbe: {}

## Custom Startup probe
##
customStartupProbeProbe: {}

## Data job template configuration options. If enabled, the custom data job template
## that is defined in the 'template' property will be applied to all data jobs.
##
## enabled  - When set to 'false', the default internal TPCS data job template will
##            be used. You can override the default template by setting this property
##            to 'true', in which case the job template is applied from the 'template'
##            property (see below).
##
## template - The K8s cronjob YAML configuration that will be used as data job template.
##            Add your K8s cronjob/job modifications in the template and then set the
##            'enabled' property to 'true' in order to override the default template.
##
datajobTemplate:
  enabled: false
  template:
    apiVersion: batch/v1beta1
    kind: CronJob
    metadata:
      annotations:                   # merged with additional annotations from TPCS
      name: cronjob-template-name    # overridden by TPCS
    spec:
      concurrencyPolicy: Forbid
      failedJobsHistoryLimit: 2
      schedule: "*/10 * * * *"       # overridden by TPCS
      startingDeadlineSeconds: 1800
      successfulJobsHistoryLimit: 1
      suspend: false                 # overridden by TPCS
      jobTemplate:
        spec:
          activeDeadlineSeconds: 43200
          backoffLimit: 3
          template:
            metadata:
              labels:                # merged with additional labels from TPCS
            spec:
              containers:            # overridden by TPCS
                - command:
                    - /bin/sh
                    - -c
                    - date; echo '************** Cronjob Template ******************'
                  name: cronjob-template-container-name
                  image: busybox
                  imagePullPolicy: IfNotPresent
              restartPolicy: OnFailure
          ttlSecondsAfterFinished: 600

# configuration for the fluentd data collector (https://github.com/vmware/kube-fluentd-operator),
# which is currently used for collecting and visualizing Versatile Data Kit logs in Kibana;
# filter and match refer to the fluentd config directives, see here https://docs.fluentd.org/configuration/config-file;
# extra refers to any additional directives you might use besides for filter and match - note that you must include
# the directive opening and closing statement in the value of fluentd.extra
# extra is included at the start of the fluentd config before the filter and match blocks
fluentd:
  enabled: false
  filter: |-
    @type parser
    key_name log
    reserve_data true
    remove_key_name_field true
    <parse>
      @type json
    </parse>
  match: |-
    @type elastic
#  extra: |-
#    <source>
#      @type http
#      port 8090
#    </source>
#    <system>
#      some system config
#    </system>

dataJob:
  readOnlyRootFileSystem: false
  executions:
    ## These values expose configuration properties for the execution api
    ## The execution API helps you collect information about previous
    ## data job executions, or manually start a job execution. Stored data job
    ## execution information will naturally increase over time. This is why we
    ## are providing a way to clean old information about data job executions through
    ## the following properties:
    ##
    ## maximumExecutionsToStore - The maximum amount of executions per data job to keep. If data
    ##                            job executions exceed this number, the older executions are deleted.
    ##
    ## executionsTtlSeconds - Total time to live per data job execution. Data job executions older than ttlSeconds
    ##              specified also get deleted.
    ##
    cleanupTask:
      ## maximum number of data job executions to store in the database / default is 100.
      maximumExecutionsToStore: "100"
      ## maximum time to live seconds for each execution / default is 14 days.
      executionsTtlSeconds: "1209600"
    logsUrl:
      ## This template will be used for building of logs URL for each data job execution returned by API.
      ## Supported variables which will be replaced in the template with the particular execution values:
      ## {{execution_id}}, {{job_name}}, {{op_id}}, {{start_time}} and {{end_time}}
      ## Example: "https://log-insight-url/li/query/stream?query=%C2%A7%C2%A7%C2%A7AND%C2%A7%C2%A7%C2%A7%C2%
      ## A7{{start_time}}%C2%A7{{end_time}}%C2%A7true%C2%A7COUNT%C2%A7text:CONTAINS:{{execution_id}}*"
      urlTemplate: ""
      ## This variable is used to determine the format of execution.startTime
      ## and execution.endTime used in the template (datajobs.executions.logsUrl.template).
      ## Supported options: iso (https://bg.wikipedia.org/wiki/ISO_8601) and unix (https://en.wikipedia.org/wiki/Unix_time).
      ## If left blank, defaults to unix.
      ## Examples: iso -> "2021-12-03T15:34:54.822098Z", unix -> "1638545973226".
      dateFormat: "unix"
      # The offset (expressed in seconds) that will be added to or subtracted from the {{start_time}}
      # variable during the building of logs URL. It could be either positive or negative number.
      # In case of positive number it will be added to the {{start_time}}.
      # In case of negative number it will be subtracted from the {{start_time}}.
      # If left blank, defaults to 0.
      startTimeOffsetSeconds: 0
      # The offset (expressed in seconds) that will be added to or subtracted from the {{end_time}}
      # variable during the building of logs URL. It could be either positive or negative number.
      # In case of positive number it will be added to the {{end_time}}.
      # In case of negative number it will be subtracted from the {{end_time}}.
      # If left blank, defaults to 0.
      endTimeOffsetSeconds: 0
  deployment:
    initContainer:
      ## Resources set on Data Job initContainer
      resources:
        limits:
          cpu: 500m
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 100Mi

  # configuration for the fluentd data collector (https://github.com/vmware/kube-fluentd-operator),
  # which is currently used for collecting Data Job logs;
  # filter and match refer to the fluentd config directives, see here https://docs.fluentd.org/configuration/config-file;
  fluentd:
    enabled: false
    filter: |-
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field true
      <parse>
        @type json
      </parse>
    match: |-
      @type elastic

# Configures the format of logs output by the control service; if set to `JSON`, logs are formatted to JSON;
# else logs are formatted to their default user-readable format
# loggingFormat: TEXT
