## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
## Set global.imageRegistry if you use docker proxy to the proxy prefix for versatiledataki.
##
global: {}
#   imageRegistry: myRegistryName
#   imagePullSecrets: # TODO enable setting imagePullSecrets so that clients can use any docker repos easier (clients can still set it up manually)
#     - myRegistryKeySecretName

# Image configuration of the Control service image. Generally those should be left as it is.
image:
   registry: registry.hub.docker.com/versatiledatakit
   repository: pipelines-control-service
   tag: "53f2621"

   ## Specify a imagePullPolicy
   ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
   ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
   ##
   pullPolicy: Always

# The image configuration for builder jobs used during deployment of data jobs.
# Generally those should be left as it is.
deploymentBuilderImage:
  registry: registry.hub.docker.com/versatiledatakit
  repository: job-builder
  tag: "1.0.5"


## String to partially override pipelines-control-service.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override pipelines-control-service.fullname template
##
# fullnameOverride:

## Service configuration
service:
   internalPort: 8092
   type: LoadBalancer # NodePort
   externalTrafficPolicy: Cluster
   # nodePort:
   # loadBalancerSourceRanges:
   # loadBalancerIP:
   # clusterIP

### Ingress resource parameters
ingress:
  ## Configure ingress resource to expose the service outside of K8s. The ingress resource will serve as reverse proxy,
  ## load balancer, terminate SSL connections and support SNI to route HTTP traffic to the local service.
  ## F.e. it could aid with the following scenario:
  ##   - HTTP/HTTPS client redirects
  ##   - Rewrite rules: rewrite URI based on regex match (see annotations document for examples)
  ##   - Single entrypoint to terminate SSL connections (ingress.tls_secret is required to support HTTPS)
  ##
  ## More in-depth documentation is available here:
  ##   https://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ## Ingress controller should already be available on the cluster. Unlike some other controllers, it is deployed
  ## separately. Chances are there is one already present.
  enabled: false
  ## Add custom annotations to the ingress resource. Objects below will update the ingress runtime configuration.
  ## Supported annotations: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/
  annotations: {}
  # kubernetes.io/ingress.class: nginx
  ## Configure hostname to use in named-based virtual host.
  host:
  ## URI path/endpoint to the service
  ## This should be unique path to instruct the ingress controller which path is served by the downstream service.
  ## Defaults to '/'
  ## This allows the service to co-exist and share the same hostname with other services.
  path: ""
  ## Provide existing secret which holds TLS certificate/key.
  ## This is will instruct the ingress controller to use specific TLS certificate on this ingress resource. By default,
  ## we use community Nginx controller which supports SNI, thus a signle IP could terminate all connections.
  ## https://github.com/kubernetes/ingress-nginx/
  tls_secret: ""

# Embedded server settings
server:
   ## Max Http Header Size - defaults to 8KB
   maxHttpHeaderSize: 8KB

credentials:
   ## Configuration for automatic management of Data Job Credentials and Users for connecting to data plane (databases usually).
   ## Then they can managed by the SRE operation team which they can do credential rotations, invalidation, etc.
   ## If enabled, each data job is automatically provisioned principal (users) and credentials (e.g passwords, keys).
   ## And upon Cloud execution they are injected into the data job and picked up by the SDK.
   ## THe following types are possible
   ## * EMPTY - no principal nor credentials are automatically provisioned. Thus effectively disables the feature. User needs to manually provision and configure credentials as job secrets.
   ## * KERBEROS - when set kerberos user and keytab file are created for each data job. kerberosXXX options below are required then.
   repository: "EMPTY"

   ## Credentials principal pattern. It must have "%(data_job)" in the pattern which will be replaced by the name of the data job..
   ## Different patterns can be given to different data-pipelines deployments that share the same authentication repository (for example same Kerberos KDC).
   ## This way they won't interfere with each other.
   ## Generally should be left as default. It should be used only if multiple deployments of Versatile Data Kit that use same authentication repository.
   ## The default principal name in VDK must be the same as this one.
   ## Or when running locally it needs to overridden (environmental variable VDK_KEYTAB_PRINCIPAL)
   ## During cloud runs that variable is set automatically.
   principalPattern: "pa__view_%(data_job)"

   ## [Required if credentials.repository is "KERBEROS" otherwise ignored]
   ## Used to create automatically users in KDC server for each data job.
   ## If left empty no users will be created for each data job. # TODO: currently operations fail though ...
   ## https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html
   kerberosKrb5Conf: ""
   ## Kerberos KDC admin server configuration and credentials
   kerberosKadminUser: ""
   kerberosKadminPassword: ""


## The owner name and email address that will be used to send all Versatile Data Kit related email notifications.
notificationOwnerEmail: "taurus@vmware.com"
notificationOwnerName: "Versatile Data Kit"

### deploymentXXX refer to properties used in order to deploy a Data Job.

## This is the kubernetes namespace where the Data Jobs will be deployed.
## If left empty it will use the same cluster as the Control Plane which is not recommended for production setup.
deploymentK8sNamespace: ""
## If you have data jobs are to be deployed into different cluster . Generally used during testing.
deploymentK8sKubeconfig: ""

## This is the kubernetes namespace builder and other system jobs used by Control Service are run
## It's is ok to leave it empty (by default).
controlK8sNamespace: ""

## Git repository where deployed data jobs source can be found and from where they will be deployed.
## If not specified, the Helm installation will create a public Github repository at
## https://github.com/versatile-data-kit-jobs with a random UUID as name.
# Git URL (without https:// scheme)
deploymentGitUrl: ""
deploymentGitRepoName: ""
deploymentGitBranch: "main"
deploymentGitRemote: "origin"
# Credentials with read-only access to the Git repository.
deploymentGitUsername: ""
deploymentGitPassword: ""
# Credentials with read and write access to the Git repository.
uploadGitReadWriteUsername: ""
uploadGitReadWritePassword: ""


## [Required] The repository where the data job images will be stored (so write permission must be granted)
## Automatically a repository will be created for each data job in ${deploymentDockerRepository}/data-job-name
## (without https:// scheme)
deploymentDockerRepository: "012285273210.dkr.ecr.us-west-2.amazonaws.com/taurus/dp"
## [Required] Public facing proxy used to when pulling data job docker images. This can be configured the same as
## deploymentDockerRepository variable below in case publicly accessible repository like Dockerhub is used
## It will be used to pull images from Data Job repos created - ${proxyRepositoryURL}/data-job-name
## The repositories should not require credentials (for creating public ECR repos see https://github.com/monken/aws-ecr-public)
## or the Kubernetes cluster and service account should be configured with appropriate authentication (set imagePullSecrets)
## (without https:// scheme)
proxyRepositoryURL: "012285273210.dkr.ecr.us-west-2.amazonaws.com/taurus/dp"

## Image registry types. Your account need to have pull and push permissions over the registry. The supported values are:
## - "ecr" - AWS ECR Registry (https://aws.amazon.com/ecr) - in this case you need to set deploymentEcr* variables above
## - "generic" : Dockerhub or Harbor type registries - in this case you need to set deploymentDockerRegistry* variables below
## If left blank defaults to ecr registry type.
deploymentDockerRegistryType: "ecr"

## [Required if deploymentDockerRegistryType=ecr]
## AWS ECR Credentials and repository to store and read images of the deployed data jobs
## The configuration should be set in case deploymentDockerRegistryType is ecr
deploymentEcrAwsRegion: ""
deploymentEcrAwsAccessKeyId: ""
deploymentEcrAwsAccessKeySecret: ""

## [Required if deploymentDockerRegistryType=generic and registry requires authentication]
## Username and Password credentials in case the deploymentDockerRegistryType is generic.
## Dockerhub account credentials or credentials of the robot account in a Harbor Project.
## The account with those credentials needs to have push and pull permissions over the registry.
deploymentDockerRegistryUsername: ""
deploymentDockerRegistryPassword: ""

## Image name of VDK which will be used to run the data jobs.
## It is recommended to use image with same tag (e.g release or latest)
## As it will be pulled before any execution thus making sure all jobs use the same and latest version of VDK.
## The image should contain installation of VDK (vdk) which will be used to run the data jobs (vdk run command)
## Only the installed python modules (vdk and its dependencies) will be used from the image.
## Everything else is effectively discarded since another image is used as base during execution.
## Override if you build VDK SDK yourself instead of using one provided by default.
deploymentVdkDistributionImage:
  registry: registry.hub.docker.com/versatiledatakit
  repository: quickstart-vdk
  tag: "release"


# The base image which will be used to create the image where data job would be run.
# On top of it the job source and its dependencies are installed for each job.
# It can be customized further by installing other native dependencies necessary to run different kinds of libraries
# For example the below default image adds support for Oracle client on top of standard python image.
# The SDK (vdk) is installed separate image (to enable zero time upgrades) see deploymentVdkDistributionImage
deploymentDataJobBaseImage:
  registry: registry.hub.docker.com/versatiledatakit
  repository: data-job-base-python-3.7
  tag: "latest"

## ini formatted options that provides default and per-job VDK options.
## Allows to provide VDK configuration via environment variables.
## They will be set as environment variables during execution of Data Job
## Enables administrators to apply common configuration for all Data Jobs
## you may use helm --set-file file.ini
## See "example-vdk-options.ini"
vdkOptions: ""

## Resources set on all data jobs by default if not overridden per job
deploymentDefaultDataJobsResources:
   limits:
      cpu: 500m
      memory: 1G
   requests:
      cpu: 500m
      memory: 1G

deploymentBuilderResourcesDefault:
   limits:
      cpu: 1000m
      memory: 1000Mi
   requests:
      cpu: 250m
      memory: 250Mi

### WebHooks configuration

webHooks:
   ## Webhooks that will delegate postCreate and postDelete actions to the devops team operating the Versatile Data Kit service.
   ## It can be configured during installation and whenever postCreate and postDelete actions need to be taken Versatile Data Kit API will call it
   ##
   ## HTTP POST request to <webhook address>/<requested_http_path> with body JSON content:
   ##
   ## * requester_user_id - this is retrieved from the Oauth2 token claims (see jwtClaimUsername configuration)
   ## * requested_resource_team - team that the job belongs to.
   ## * requested_resource_name - this is always "data-job"
   ## * requested_resource_id - the data job name
   ## * requested_http_path - The HTTP Path of the REST API request being invoked.
   ##                         For Create Job: it will be /data-jobs/for-team/{team-name}/jobs.
   ##                         For Delete Job: it will be /data-jobs/for-team/{team-name}/jobs/{job-name}.
   ##                         Example: /data-jobs/for-team/taurus/jobs/example.
   ## * requested_http_verb - The HTTP verb of the REST API request being invoked
   ##                         For Create Job: it will be POST.
   ##                         For Delete Job: it will be DELETE.
   ## Complete example:
   ### Post Delete WebHook:
   ### HTTP POST: https://webhook.com/data-jobs/for-team/taurus/jobs/example
   ### {
   ###    requester_user_id: auserov,
   ###    requested_resource_team: taurus,
   ###    requested_resource_name: data-job,
   ###    requested_resource_id: example,
   ###    requested_http_path: /data-jobs/for-team/taurus/jobs/example,
   ###    requested_http_verb: DELETE
   ###  }
   ###
   ### Post Create WebHook:
   ### HTTP POST: https://webhook.com/data-jobs/for-team/taurus/jobs
   ### {
   ###    requester_user_id: auserov,
   ###    requested_resource_team: taurus,
   ###    requested_resource_name: data-job,
   ###    requested_resource_id: example,
   ###    requested_http_path: /data-jobs/for-team/taurus/jobs,
   ###    requested_http_verb: POST
   ###  }
   ###
   ## Expected return results(for both the request types):
   ##    2xx means the web Hook Operation is successful and the job is created/deleted
   ##    4xx or 5xx the job is not created/deleted
   ## 4xx response content is shown to end user as an error message.
   ## 5xx responses will be retried (number of retries is based on configuration, defaults to 3)
   postCreate:
      webhookUri: ""
      internalErrorsRetries: 3
   postDelete:
      webhookUri: ""
      internalErrorsRetries: 3

### Security configuration

security:
   ## Feature flag to enable/disable security entirely
   enabled: false
   oauth2:
      ## [ Required if security.enabled = True ]
      ## The JSON Web Key Set (JWKS) is a set of keys which contains the public keys
      ## used to verify any JSON Web Token (JWT) issued by the authorization server
      ## Required if security is enabled
      ## See https://tools.ietf.org/html/rfc7517
      ## For example :
      ##    https://www.googleapis.com/oauth2/v3/certs
      ##    https://console.cloud.vmware.com/csp/gateway/am/api/auth/token-public-key?format=jwks
      ## JWT access token need be passed on requests in 'Authorization: Bearer' header
      jwtJwkSetUri: ""

   ## Feature flag to enable/disable authorization
   authorizationEnabled: false
   authorization:
      ## [ Required if security.authorizationEnabled = True ]
      ## Webhook that will delegate authorization decisions.
      ## It can be configured during   installation and whenever authorization decision needs to be taken Versatile Data Kit API will call it
      ##
      ## HTTP POST request to webhook address with body JSON content:
      ##
      ## * requester_user_id - this is retrieved from the Oauth2 token claims (see jwtClaimUsername configuration)
      ## * requested_resource_team - team that the job belongs to configured during its creation or passed to create job request. Otherwise if job does not exists, it's empty.
      ## * requested_resource_name - this is always "data-job"
      ## * requested_resource_id - the data job name
      ## * requested_permission - it is "read" for operations that are READ only (e.g. HTTP GET) - "write" if modification is done
      ## * requested_http_path - The HTTP Path of the REST API request being authorized
      ## * requested_http_verb - The HTTP verb of the REST API request being authorized
      ##
      ## For example:
      ## {
      ##    requester_user_id: auserov,
      ##    requested_resource_team: job-team,
      ##    requested_resource_name: data-job,
      ##    requested_resource_id: example,
      ##    requested_permission: write
      ##    requested_http_path: /data-job/example/deployments,
      ##    requested_http_verb: POST
      ## }
      ## Expected return results:
      ##    2xx means access is granted,
      ##    4xx or 5xx access is rejected.
      ## 4xx response content is shown to end user as an error message.
      ## 5xx responses will be retried
      webhookUri: ""
      ## What JWT token claim (aka attribute/field) will fetch the username from.
      jwtClaimUsername: "username"
## TODO: add support for OpenIdConnect ID_TOKEN - Contact Taurus team if necessary

## Number of replicas of the Control Plane Deployment
replicas: 2
## Resources set on Control Plane Deployment
resources:
   limits:
      cpu: 500m
      memory: 500Mi
   requests:
      cpu: 500m
      memory: 500Mi

## [Required] Database configuration used by Control Plane
## Database is used to store Data Jobs metadata like name, team, description
## and some deploy specific configuration like execution schedule
## Database schema is internal implementation detail and should not be queried directly.

## Configuration for the embedded database, set cockroachdb.enabled to false if you want to use external database
## See https://github.com/cockroachdb/helm-charts/tree/master/cockroachdb
cockroachdb:
   enabled: true
   storage:
      persistentVolume:
         size: 1Gi
   statefulset:
      replicas: 3
      resources:
         limits:
            cpu: 500m
            memory: 500Mi
         requests:
            cpu: 500m
            memory: 500Mi
   init:
      resources:
         limits:
            cpu: 500m
            memory: 500Mi
         requests:
            cpu: 500m
            memory: 500Mi
   service:
      public:
         type: ClusterIP

## Configuration for external CockroachDB database
##
## If cockroachdb.enabled is true - it is ignored and embedded database is used (using default credentials)
## If embedded cockroach db is disabled either provide existing secret (database.secretName) or specify credentials.
## If both are set, externalSecretName will be used.
database:
   ## Name of the secret which holds JDBC credentials. The chart will not attempt to create this, but will use it as is.
   ## The secret should contain keys: JDBC, USERNAME, PASSWORD
   externalSecretName: ""
   ## Alternatively provide JDBC url, username and password. externalSecretName takes precendence if both are set.
   jdbcUrl: ""
   username: ""
   password: ""
## Prometheus exporter configuration
##
metrics:
   enabled: true
   ## Prometheus Operator ServiceMonitor configuration
   ##
   serviceMonitor:
      enabled: false
      ## Namespace in which Prometheus is running
      ##
      # namespace: monitoring

      ## Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: 30s

      ## Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: 10s

      ## Other options also can be set
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      honorLabels: false
      scheme: http
      # params:

      ## Custom labels added to the ServiceMonitor usually necessary by Prometheus to find the ServiceMonitor
      # selector

      ## Drop the execution_id label of the taurus_datajob_termination_status metrics to reduce its cardinality.
      ## Currently it is not being used by the Prometheus rules below.
      metricRelabelings:
         -  regex: 'execution_id'
            action: labeldrop

## Alerting configuration
##
alerting:
   enabled: false

   ## Custom PrometheusRule to be defined
   ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
   ##
   prometheusRule:
      additionalLabels: { }
      ## Specify the namespace where Prometheus Operator is running
      ##
      # namespace: monitoring

      ## Define individual alerting rules as required
      ## ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
      ##
      ## Below are some built-in rules that can be used as examples.
      ## The rules rely on the taurus_datajob_termination_status and taurus_datajob_info metrics (ref: README.md)
      ## as well as the kube_cronjob_next_schedule_time, kube_cronjob_info, and kube_job_status_completion_time
      ## metrics exposed by kube_state_metrics (ref: https://github.com/kubernetes/kube-state-metrics).
      rules:
         JobSuccess:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: |-
                  {{`{{ $labels.email_notified_on_success | reReplaceAll ";" "," }}`}}
               subject: |-
                  {{`[success][data job run] {{ $labels.data_job }}`}}
               content: |-
                  {{`Dear Versatile Data Kit user,<br>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> succeeded.
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: |-
               {{`label_replace(
                        (avg by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_success)
                        avg by(data_job, email_notified_on_success) (taurus_datajob_info{email_notified_on_success!=""}) == bool 0)
                    * on(data_job) group_left(job_name)
                      topk by(data_job) (1, label_replace(kube_job_status_completion_time, "data_job", "$1", "job_name", "(.*)-latest.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)")`}}
         JobDelay:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: |-
                  {{`{{ $labels.email_notified_on_platform_error | reReplaceAll ";" "," }}`}}
               subject: |-
                  {{`[platform error][data job delay] {{ $labels.data_job }}`}}
               content: |-
                  {{`Dear Versatile Data Kit user,<br>
                  Your data job <strong>{{ $labels.data_job }}</strong> was delayed for more than {{ $value | printf "-(%f) / 60" | query | first | value | printf "%.f" }} minutes.
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: |-
               {{`label_replace(
                    (kube_cronjob_next_schedule_time > 0) - time(),
                    "data_job", "$1", "cronjob", "(.+)-latest")
                  * on(data_job) group_left(email_notified_on_platform_error)
                    avg by(data_job, email_notified_on_platform_error) (taurus_datajob_info{email_notified_on_platform_error!=""})
               <
                  on(data_job)
                  group_right(email_notified_on_platform_error)
                  -(min by(data_job) (taurus_datajob_notification_delay) * 60)`}}
         JobFailurePlatform:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: |-
                  {{`{{ $labels.email_notified_on_platform_error | reReplaceAll ";" "," }}`}}
               subject: |-
                  {{`[platform error][data job run] {{ $labels.data_job }}`}}
               content: |-
                  {{`Dear Versatile Data Kit user,<br>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> has failed.
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: |-
               {{`label_replace(
                        (avg by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_platform_error)
                        avg by(data_job, email_notified_on_platform_error) (taurus_datajob_info{email_notified_on_platform_error!=""}) == bool 1)
                    * on(data_job) group_left(job_name)
                      topk by(data_job) (1, label_replace(kube_job_status_completion_time, "data_job", "$1", "job_name", "(.*)-latest.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)")`}}
         JobFailureUser:
            enabled: true
            for: 1m
            labels:
               severity: info
               source: data-pipelines
            annotations:
               email_to: |-
                  {{`{{ $labels.email_notified_on_user_error | reReplaceAll ";" "," }}`}}
               subject: |-
                  {{`[user error][data job run] {{ $labels.data_job }}`}}
               content: |-
                  {{`Dear Versatile Data Kit user,<br>
                  The last execution of your data job <strong>{{ $labels.data_job }}</strong> has failed.
                  <p>
                  The sender mailbox is not monitored, please do not reply to this email.
                  </p>
                  Best,<br>
                  Versatile Data Kit Support`}}
            expr: |-
               {{`label_replace(
                        (avg by(data_job) (taurus_datajob_termination_status)
                      * on(data_job) group_left(email_notified_on_user_error)
                        avg by(data_job, email_notified_on_user_error) (taurus_datajob_info{email_notified_on_user_error!=""}) == bool 3)
                    * on(data_job) group_left(job_name)
                      topk by(data_job) (1, label_replace(kube_job_status_completion_time, "data_job", "$1", "job_name", "(.*)-latest.*")) != 0,
                    "execution_id", "$1", "job_name", "(.*)")`}}

## Synchronization mechanism for consistent metrics. The time format is number in milliseconds
monitoringSync:
   ## Initial delay between service startup and first update of metrics
   initialDelayMillis: 10000
   ## Interval in which a single instance of the service will synchronize its metrics from centralized location
   ## Note: this should be lower than the prometheus scrape configuration so we make sure every service exposes
   ## latest monitoring metrics for prometheus to scrape
   delayMillis: 60000

## Role Based Access in Kubernetes
## ref: https://kubernetes.io/docs/admin/authorization/rbac/
## The Control service needs permission to create job.batch API resource.
## Generally those should be left as it is.
rbac:
   ## Specifies whether RBAC rules should be created
   ## binding Versatile Data Kit Control Service Kubernetes ServiceAccount to a role
   ## that allows Control Service pods querying the K8s API
   create: true
   ## Specifies whether RBAC rules should be created for Data Jobs deployment namespace (see deploymentK8sNamespace)
   ## If this is set to false - deploymentK8sKubeconfig must be set with required permissions (see role_datajobs.yaml)
   datajobsDeployment:
      create: true

## Versatile Data Kit Control Service pods ServiceAccount
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
   ## Specifies whether a ServiceAccount should be created
   ##
   create: true
   ## The name of the ServiceAccount to use.
   ## If not set and create is true, a name is generated using the pipelines-control-service.fullname template
   ##
   # name:


## This is deprecated ; use extraEnvVars
# extraVars:
# - name: EXTRA_VAR_1
#   value: extra-var-value-1
# - name: EXTRA_VAR_2
#   value: extra-var-value-2

## Pass extra environment variables to the Versatile Data Kit Control Service container.
#extraEnvVars:
#  EXTRA_VAR_1: extra-var-value-1
#  EXTRA_VAR_2: extra-var-value-2


## Configure extra options for liveness and readiness and startup probes
## Control Services exposes  /data-jobs/debug/health to unauthenticated requests, making it a good
## default liveness and readiness path. However, that may not always be the
## case. For example, if the image value is overridden to an image containing a
## module that alters that route.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
##
livenessProbe:
   enabled: true
   path: /data-jobs/debug/health/liveness
   initialDelaySeconds: 120
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 4
   successThreshold: 1
readinessProbe:
   enabled: true
   path: /data-jobs/debug/health/readiness
   initialDelaySeconds: 30
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 4
   successThreshold: 1
startupProbe:
   enabled: true
   path: /data-jobs/debug/health/liveness
   initialDelaySeconds: 30
   periodSeconds: 15
   timeoutSeconds: 10
   failureThreshold: 30
   successThreshold: 1

## Custom Liveness probe
##
customLivenessProbe: {}

## Custom Readiness probe
##
customReadinessProbe: {}

## Custom Startup probe
##
customStartupProbeProbe: {}

## Data job template configuration options. If enabled, the custom data job template
## that is defined in the 'template' property will be applied to all data jobs.
##
## enabled  - When set to 'false', the default internal TPCS data job template will
##            be used. You can override the default template by setting this property
##            to 'true', in which case the job template is applied from the 'template'
##            property (see below).
##
## template - The K8s cronjob YAML configuration that will be used as data job template.
##            Add your K8s cronjob/job modifications in the template and then set the
##            'enabled' property to 'true' in order to override the default template.
##
datajobTemplate:
  enabled: false
  template:
    apiVersion: batch/v1beta1
    kind: CronJob
    metadata:
      annotations:                   # merged with additional annotations from TPCS
      name: cronjob-template-name    # overridden by TPCS
    spec:
      concurrencyPolicy: Forbid
      failedJobsHistoryLimit: 2
      schedule: "*/10 * * * *"       # overridden by TPCS
      startingDeadlineSeconds: 1800
      successfulJobsHistoryLimit: 1
      suspend: false                 # overridden by TPCS
      jobTemplate:
        spec:
          activeDeadlineSeconds: 129600
          backoffLimit: 3
          template:
            metadata:
              labels:                # merged with additional labels from TPCS
            spec:
              containers:            # overridden by TPCS
                - command:
                    - /bin/sh
                    - -c
                    - date; echo '************** Cronjob Template ******************'
                  name: cronjob-template-container-name
                  image: busybox
                  imagePullPolicy: IfNotPresent
              restartPolicy: OnFailure
          ttlSecondsAfterFinished: 600

# configuration for the fluentd data collector (https://github.com/vmware/kube-fluentd-operator),
# which is currently used for collecting and visualizing Versatile Data Kit logs in Kibana;
# filter and match refer to the fluentd config directives, see here https://docs.fluentd.org/configuration/config-file;
# extra refers to any additional directives you might use besides for filter and match - note that you must include
# the directive opening and closing statement in the value of fluentd.extra
# extra is included at the start of the fluentd config before the filter and match blocks
fluentd:
  enabled: false
  filter: |-
    @type parser
    key_name log
    reserve_data true
    remove_key_name_field true
    <parse>
      @type json
    </parse>
  match: |-
    @type elastic
#  extra: |-
#    <source>
#      @type http
#      port 8090
#    </source>
#    <system>
#      some system config
#    </system>

## These values expose configuration properties for the execution api
## The execution API helps you collect information about previous
## data job executions, or manually start a job execution. Stored data job
## execution information will naturally increase over time. This is why we
## are providing a way to clean old information about data job executions through
## the following properties:
##
## maximumExecutionsToStore - The maximum amount of executions per data job to keep. If data
##                            job executions exceed this number, the older executions are deleted.
##
## ttlSeconds - Total time to live per data job execution. Data job executions older than ttlSeconds
##              specified also get deleted.
##
dataJobExecutionsCleanupTask:
  ## maximum number of data job executions to store in the database / default is 100.
  maximumExecutionsToStore: "100"
  ## maximum time to live seconds for each execution / default is 14 days.
  executionsTtlSeconds: "1209600"
