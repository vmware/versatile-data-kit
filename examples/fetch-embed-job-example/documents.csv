This is a test Confluence space that provides some vSAN documentation.
"Verify that the ESXi hosts in your organization meet the vSAN hardware requirements. Storage Device Requirements All capacity devices, drivers, and firmware versions in your vSAN configuration must be certified and listed in the vSAN section of the VMware Compatibility Guide. Storage Device Requirements for vSAN Hosts Storage Component Requirements Cache One SAS or SATA solid-state disk (SSD) or PCIe flash device. Before calculating the Primary level of failures to tolerate , check the size of the flash caching device in each disk group. For hybrid cluster, it must provide at least 10 percent of the anticipated storage consumed on the capacity devices, not including replicas such as mirrors. vSphere Flash Read Cache must not use any of the flash devices reserved for vSAN cache. The cache flash devices must not be formatted with VMFS or another file system. Virtual machine data storage For hybrid group configuration, make sure that at least one SAS or NL-SAS magnetic disk is available. For all-flash disk group configuration, make sure at least one SAS, or SATA solid-state disk (SSD), or PCIe flash device. Storage controllers One SAS or SATA host bus adapter (HBA), or a RAID controller that is in passthrough mode or RAID 0 mode. To avoid issues, consider these points when the same storage controller is backing both vSAN and non-vSAN disks: Do not mix the controller mode for vSAN and non-vSAN disks to avoid handling the disks inconsistently, which can negatively impact vSAN operation. If the vSAN disks are in RAID mode, the non-vSAN disks must also be in RAID mode. When you use non-vSAN disks for VMFS, use the VMFS datastore only for scratch, logging, and core dumps. Do not run virtual machines from a disk or RAID group that shares its controller with vSAN disks or RAID groups. Do not passthrough non-vSAN disks to virtual machine guests as Raw Device Mappings (RDMs). For more information, see https://kb.vmware.com/s/article/2129050 . To learn about controller supported features, such as passthrough and RAID, refer to the vSAN HCL: https://www.vmware.com/resources/compatibility/search.php?deviceCategory=vsan Memory The memory requirements for vSAN depend on the number of disk groups and devices that the ESXi hypervisor must manage. For more information, see the VMware knowledge base article at https://kb.vmware.com/s/article/2113954 . Flash Boot Devices During installation, the ESXi installer creates a coredump partition on the boot device. The default size of the coredump partition satisfies most installation requirements. If the memory of the ESXi host has 512 GB of memory or less, you can boot the host from a USB, SD, or SATADOM device. When you boot a vSAN host from a USB device or SD card, the size of the boot device must be at least 4 GB. If the memory of the ESXi host has more than 512 GB, consider the following guidelines. You can boot the host from a SATADOM or disk device with a size of at least 16 GB. When you use a SATADOM device, use a single-level cell (SLC) device. If you are using vSAN 6.5 or later, you must resize the coredump partition on ESXi hosts to boot from USB/SD devices. For more information, see the VMware knowledge base article at http://kb.vmware.com/kb/2147881 . When you boot an ESXi 6.0 or later host from USB device or from SD card, vSAN trace logs are written to RAMDisk. These logs are automatically offloaded to persistent media during shutdown or system crash (panic). This is the only support method for handling vSAN traces when booting an ESXi from a USB stick or SD card. If a power failure occurs, vSAN trace logs are not preserved. When you boot an ESXi 6.0 or later host from a SATADOM device, vSAN trace logs are written directly to the SATADOM device. Therefore it is important that the SATADOM device meets the specifications outlined in this guide."
"VMware vSphere is VMware's virtualization platform, which transforms data centers into aggregated computing infrastructures that include CPU, storage, and networking resources. vSphere manages these infrastructures as a unified operating environment, and provides you with the tools to administer the data centers that participate in that environment. The two core components of vSphere are ESXi and vCenter Server. ESXi is the virtualization platform where you create and run virtual machines and virtual appliances. vCenter Server is the service through which you manage multiple hosts connected in a network and pool host resources. Want to know what is in the current release of vSphere? Look at the latest vSphere release notes. Learn About Some of Our Features You can only deploy or upgrade to vCenter Server using an appliance. The vCenter Server appliance contains all the Platform Services Controller services from earlier releases, preserving all previous functionality, including authentication, certificate management , and licensing. vSphere 8.0 introduces vSphere Distributed Services Engine that provides the ability to offload some of the infrastructure functions from a server CPU to a data processing unit (DPU). A DPU device features multiple embedded CPU cores, memory, and a trimmed version of an ESXi hypervisor that runs on the server. DPUs are pre-configured devices that you can use for network acceleration. You manage the life cycle of the software and firmware that runs on a DPU device by using vSphere Lifecycle Manager images workflows. Use vSphere Lifecycle Manager as a centralized and simplified lifecycle management mechanism for VMware ESXi hosts. With vSphere Lifecycle Manager you can manage ESXi hosts by using images and baselines at the cluster level. You can also convert a cluster that uses baselines into a cluster that uses vSphere Lifecycle Manager images . In vSphere 8.0 you can start using vSphere Configuration Profiles to manage the configuration of the hosts in a cluster collectively. You first set up a desired configuration at the cluster level and then, in a single operation, you can apply the configuration to all hosts in the cluster. Learn how to use vSphere with Tanzu to transform vSphere into a platform for running Kubernetes workloads natively on the hypervisor layer. With this functionality, you can enable a vSphere cluster to run Kubernetes workloads by configuring it then as a Supervisor . DevOps engineers and application developers can then run containerized applications on vSphere Namespaces by deploying vSphere Pod , VMs, and upstream Kubernetes clusters through Tanzu Kubernetes Grid with vSphere . You can deploy a Supervisor on vSphere Zones to provide high-availability to your workloads at cluster level as one vSphere Zone maps to one vSphere cluster. Workloads that you run on a Supervisor deployed on zones are distributed on the vSphere clusters that are part of the zones and are protected against cluster-level failure. To keep your vSphere with Tanzu environment up to date, backup and restore, and troubleshoot, learn how to maintain vSphere with Tanzu . You can view available vCenter Server updates and upgrades and produce interoperability reports about VMware products associated with vCenter Server using Update Planner. You can also generate pre-update reports that let you make sure your system meets the minimum software and hardware requirements for a successful upgrade of vCenter Server. The report provides information about problems that might prevent the completion of a software upgrade, and actions you can take to remedy those problems. You can use centralized license management to manage licenses for ESXi hosts, vCenter Server, vSAN clusters, and other VMware solutions. Learn how to use the VMware vSphere Client to manage licenses in your vCenter Server environment . Learn how to configure networking for vSphere, including how to create vSphere distributed switches and vSphere standard switches , monitor networks to analyze the traffic between virtual machines (VMs) and hosts , and manage network resources . vSphere networking is one of the most critical components in your environment, as it is how your ESXi hosts and VMs communicate. You can learn about vSphere storage to help you plan a storage strategy for your virtual data center. You can also learn how to configure and use the virtualized and software-defined storage technologies that ESXi and vCenter Server provide. vSphere supports several storage technologies for both traditional and software-defined storage environments. Learn how to secure your environment using vSphere security features and best practices to safeguard your environment from attack. vSphere provides comprehensive, built-in security, delivering secure applications, infrastructure, data, and access. You can provide business continuity using vCenter High Availability (vCenter HA) and vSphere Fault Tolerance (FT). vCenter HA provides failover protection against hardware and operating system outages within your virtualized IT environment. If there is a host failure, Fault Tolerance provides continuous protection for a VM. You can use resource pools, clusters, vSphere Distributed Resource Scheduler (DRS), vSphere Distributed Power Management (DPM), and vSphere Storage I/O Control to manage and allocate resources for ESXi hosts and vCenter Server. Try Our Deployment and Planning Tools The following resources are designed to help you plan your vSphere data center deployment, and effectively manage your vSphere environment. vSphere Hardware and Guest Operating System Compatibility Guides . An online reference that shows what hardware, converged systems, operating systems, third-party applications, and VMware products are compatible with a specific version of a VMware software product. VMware Product Interoperability Matrices . Provides details about the compatibility of current and earlier versions of VMware vSphere components, including ESXi, vCenter Server, and other VMware products. VMware Configuration Maximums . When you configure, deploy, and operate your virtual and physical equipment, you must stay at or below the maximums supported by your product. The limits presented in the Configuration Maximums tool are tested limits supported by VMware. Access Developer and Automation Documentation VMware Developer is a website dedicated to our developer and automation community. To learn about vSphere APIs, SDKs, and command-line interfaces, visit these VMware Developer resources: VMware SDKs VMware APIs VMware command-line interfaces under Automation Tools Explore Our Videos You can learn about deploying, managing, and administering vSphere by reading the documentation, and by watching videos on the VMware Information Experience video channel . Learn More About vSphere To learn about vSphere and data center virtualization, see the following resources. Learn more about vSphere by visiting the vSphere Product Page . Ask questions about vSphere by visiting the vSphere Community Forum . You can get help, opinions, and feedback from other VMware users by participating in the discussion forums. Explore vSphere without having to install it using the VMware vSphere Hands-on Labs environment. Learn about the solutions vSphere provides to help you overcome your IT challenges, and create a more efficient digital infrastructure by visiting Cloud Platform Tech Zone and vSphere White Papers and Technical Notes . Read the latest products announcements, technical articles, and operations guidance from VMware on the vSphere Blog . Learn about benchmarking, performance architectures, and other performance-focused topics at the blog VMware VROOM! , maintained by VMware's Performance Engineering team. Visit the William Lam blog, which focuses on automation, integration, and operation of the VMware Software Defined Datacenter (SDDC). William Lam is a Staff Solutions Architect working at VMware. Use vSphere Documentation The vSphere documents in HTML reflect the latest vSphere update release of each major vSphere version. For example, version 8.0 contains all the updates for 8.0.x releases. All our documentation comes in PDF format, which you can access by selecting the Download PDF icon on any page in the HTML documentation. PDFs for previous releases of vSphere are available for download in a ZIP archive format. The archive can be found under the Archive Packages heading for each major version in the table of contents on the left. You can create custom documentation collections, containing only the content that meets your specific information needs, using MyLibrary ."
"If you are considering vSAN, you can choose from more than one configuration solution for deploying a vSAN cluster. Depending on your requirement, you can deploy vSAN in one of the following ways. vSAN ReadyNode The vSAN ReadyNode is a preconfigured solution of the vSAN software provided by VMware partners, such as Cisco, Dell, Fujitsu, IBM, and Supermicro. This solution includes validated server configuration in a tested, certified hardware form factor for vSAN deployment that is recommended by the server OEM and VMware. For information about the vSAN ReadyNode solution for a specific partner, visit the VMware Partner website. User-Defined vSAN Cluster You can build a vSAN cluster by selecting individual software and hardware components, such as drivers, firmware, and storage I/O controllers that are listed in the vSAN Compatibility Guide (VCG) website at http://www.vmware.com/resources/compatibility/search.php . You can choose any servers, storage I/O controllers, capacity and flash cache devices, memory, any number of cores you must have per CPU, that are certified and listed on the VCG website. Review the compatibility information on the VCG website before choosing software and hardware components, drivers, firmware, and storage I/O controllers that vSAN supports. When designing a vSAN cluster, use only devices, firmware, and drivers that are listed on the VCG website. Using software and hardware versions that are not listed in the VCG might cause cluster failure or unexpected data loss. For information about designing a vSAN cluster, see Designing and Sizing a vSAN Cluster ."
"This topic discusses the limitations of vSAN. When working with vSAN, consider the following limitations: vSAN does not support hosts participating in multiple vSAN clusters. However, a vSAN host can access other external storage resources that are shared across clusters. vSAN does not support vSphere DPM and Storage I/O Control. vSAN does not support SE Sparse disks. vSAN does not support RDM, VMFS, diagnostic partition, and other device access features."
"What's in the Release Notes These release notes introduce you to new features in VMware vSAN 8.0 Update 2 and provide information about known issues. What's New vSAN 8.0 Update 2 introduces the following new features and enhancements: Disaggregated Storage Enhanced topologies for disaggregation with vSAN Express Storage Architecture bring feature parity for vSAN OSA and vSAN ESA. vSAN ESA support for stretched clusters in disaggregated topology . vSAN ESA supports disaggregation when using vSAN stretched clusters. In addition to supporting several stretched cluster configurations, vSAN also optimizes the network paths for certain topologies to improve the performance capabilities of stretched cluster configurations. Support of disaggregation across clusters using multiple vCenter Servers . vSAN 8.0 Update 2 supports disaggregation across environments using multiple vCenter Servers when using vSAN ESA. This enables vSphere or vSAN clusters managed by one vCenter Server to use the storage resources of a vSAN cluster managed by a different vCenter Server.. vSAN ESA Adaptive Write path for disaggregated storage . Disaggregated deployments get the performance benefits of a new adaptive write path previously introduced in vSAN 8.0 Update 1 for standard ESA based deployments. VMs running on a vSphere or vSAN cluster that consume storage from another vSAN ESA cluster can take advantage of this capability. Adaptive write path technology in a disaggregated environment helps your VMs achieve higher throughput and lower latency, and do so automatically in real time, without any interaction by the administrator. Core Platform Enhancements Integrated File Services for Cloud Native and traditional workloads . vSAN 8.0 Update 2 supports vSAN File Service on vSAN Express Storage Architecture. File service clients can benefit from performance and efficiency enhancements provided by vSAN ESA. Adaptive Write Path optimizations in vSAN ESA . vSAN ESA introduces an adaptive write path that helps the cluster ingest and process data more quickly. This optimization improves performance for workloads driving high I/O to single object (VMDK), and also improves aggregate cluster performance. Increased number of VM's per host in vSAN ESA clusters (up to 500/host) . vSAN 8.0 Update 2 supports up to 500 VMs per host VM on vSAN ESA clusters, provided the underlying hardware infrastructure can support it. Now you can leverage NVMe-based high performance hardware platforms optimized for the latest generation of CPUs with high core densities, and consolidate more VMs per host. New ReadyNode profile and support for read-intensive devices for vSAN ESA . vSAN ESA announces the availability of new ReadyNode profiles designed for small data centers and edge environments with lower overall hardware requirements on a per-node basis. This release also introduces support for read-intensive storage devices. vSAN ESA support for encryption deep rekey . vSAN clusters using data-at-rest encryption have the ability to perform a deep rekey operation. A deep rekey decrypts the data that has been encrypted and stored on a vSAN cluster using the old encryption key, and re-encrypts the data using newly issued encryption keys prior to storing it on the vSAN cluster. Enriched Operations vSAN ESA prescriptive disk claim . vSAN ESA includes a prescriptive disk claim process that further simplifies management of storage devices in each host in a vSAN cluster. This feature provides consistency to the disk claiming process during initial deployment and cluster expansion. Capacity reporting enhancements . Overhead breakdown in vSAN ESA space reporting displays both the ESA object overhead and the original file system overhead. Auto-Policy management improvements in vSAN ESA . Enhanced auto-policy management feature determines if the default storage policy needs to be adjusted when a user adds or removes a host from a cluster. If vSAN identifies a need to change the default storage policy, it triggers a health check warning. You can make the change with a simple click at which time vSAN reconfigures the cluster with the new policy. Skyline Health remediation enhancements . vSAN Skyline Health helps you reduce resolution times by providing deployment-specific guidance along with more prescriptive guidance on how to resolve issues. Key expiration for clusters with data-at-rest encryption . vSAN 8.0 Update 2 supports the use of KMS servers with a key expiration attribute used for assigning an expiration date to a Key Encryption Key (KEK). I/O top contributors enhancements . vSAN Performance Service has improved the process to find performance hot spots over a customizable time period to help you diagnose performance issues while using multiple types of sources for analysis (VMs, host disks, and so on). I/O Trip Analyzer supported on two node clusters and stretched clusters . vSAN 8.0 Update 2 has enhanced the I/O Trip Analyzer to report on workloads in a vSAN stretched cluster. Now you can determine where the primary source of latency is occurring in a vSAN stretched cluster, as well as latencies in other parts of the stack that can contribute to the overall latency experienced by the VM. Easier configuration for two node clusters and stretched clusters . Several new features to help management of two node and stretched cluster deployments. Witness host traffic configured in the vSphere Client.​ Support for medium sized witness host appliance in vSAN ESA. Support in vLCM to manage lifecycle of shared witness host appliance types. Cloud Native Storage CSI snapshot support for TKG service . Cloud Native Storage introduces CSI snapshot support for TKG Service, enabling K8s users and backup vendors to take persistent volume snapshots on TKGS. Data mobility of Cloud Native persistent volumes across datastores . This release introduces built-in migration of persistent volumes across datastores in the vSphere Client. VMware vSAN Community Use the vSAN Community Web site to provide feedback and request assistance with any problems you find while using vSAN. Upgrades for This Release For instructions about upgrading vSAN, see the VMware vSAN 8.0 Update 2 documentation . Note : Before performing the upgrade, please review the most recent version of the VMware Compatibility Guide to validate that the latest vSAN version is available for your platform. Note : vSAN Express Storage Architecture is available only for new deployments. You cannot upgrade a cluster to vSAN ESA. vSAN 8.0 Update 2 is a new release that requires a full upgrade to vSphere 8.0 Update 2. Perform the following tasks to complete the upgrade: Upgrade to vCenter Server 8.0 Update 2. For more information, see the VMware vSphere 8.0 Update 2 Release Notes . Upgrade hosts to ESXi 8.0 Update 2. For more information, see the VMware vSphere 8.0 Update 2 Release Notes . Upgrade the vSAN on-disk format to version 19.0. If upgrading from on-disk format version 3.0 or later, no data evacuation is required (metadata update only). Upgrade FSVM to enable new File Service features and get all the latest updates. Note : vSAN retired disk format version 1.0 in vSAN 7.0 Update 1. Disks running disk format version 1.0 are no longer recognized by vSAN. vSAN will block upgrade through vSphere Update Manager, ISO install, or esxcli to vSAN 7.0 Update 1. To avoid these issues, upgrade disks running disk format version 1.0 to a higher version. If you have disks on version 1.0, a health check alerts you to upgrade the disk format version. Disk format version 1.0 does not have performance and snapshot enhancements, and it lacks support for advanced features including checksum, deduplication and compression, and encryption. For more information about vSAN disk format version, see KB 2148493 . Upgrading the On-disk Format for Hosts with Limited Capacity During an upgrade of the vSAN on-disk format from version 1.0 or 2.0, a disk group evacuation is performed. The disk group is removed and upgraded to on-disk format version 17.0, and the disk group is added back to the cluster. For two-node or three-node clusters, or clusters without enough capacity to evacuate each disk group, select Allow Reduced Redundancy from the vSphere Client. You also can use the following RVC command to upgrade the on-disk format: vsan.ondisk_upgrade --allow-reduced-redundancy When you allow reduced redundancy, your VMs are unprotected for the duration of the upgrade, because this method does not evacuate data to the other hosts in the cluster. It removes each disk group, upgrades the on-disk format, and adds the disk group back to the cluster. All objects remain available, but with reduced redundancy. If you enable deduplication and compression during the upgrade, you can select Allow Reduced Redundancy from the vSphere Client. Limitations For information about maximum configuration limits for the vSAN 8.0 Update 2 release, see the Configuration Maximums documentation. Known Issues Failed to consolidate vmdk of vRDM on vSAN ESA cluster This issue can occur if you use a vRDM type virtual disk with compatibility mode set to virtual and disk mode set to dependent. In the vSAN ESA datastore, the corresponding virtual disk cannot be consolidated in the snapshot deletion operation after the snapshot is created. This problem can cause the virtual disk to fail to create new snapshot. None. Encryption operations blocked if Native Key Provider is not backed up If you are using Native Key Provider for vSAN data-at-rest encryption, and the Native Key Provider is not backed up, encryption-related reconfiguration operations might fail with the following message: The KMS cluster {kmsCluster} is a Native Key Provider that is not backed up yet . Back up the Native Key Provider before enabling data-at-rest encryption. Must manually assign storage policy to restored VMs If you restore or create a linked clone from a deleted VM, vSAN does not automatically assign a storage policy. The new VM does not have a storage policy. Manually assign the storage policy after you create a linked clone VM or restore a VM. Japanese text for Skyline Health displays incorrectly when vCenter is air gapped This issue can occur when the vCenter network connection is air gapped. Some Japanese text for Skyline Health localizes incorrectly. You might see a message ID such as the following: com.vmware.vsan.health.XXX Perform the following steps: SSH to vCenter. Open the following file: /etc/vmware-vsan-health/cloudHealthResources/locale/ja/vsanhealthremediation.vmsg Search for the following: vsan.health.optimaldsdefaultpolicy.htf.0.default.0.des Merge the two lines into one as shown below, and save the file. Restart the vSAN health service: /usr/sbin/vmon-cli -r vsan-health Before: vsan.health.optimaldsdefaultpolicy.htf.0.default.0.des=健全性テーブルに表示される vSAN の最適なデータストアのデフォルト ポリシーの「許容される障害の数」または「サイトの耐障害性」の構成 (あるいは両方) が推奨ポリシーと一致しません。 After: vsan.health.optimaldsdefaultpolicy.htf.0.default.0.des=健全性テーブルに表示される vSAN の最適なデータストアのデフォルト ポリシーの「許容される障害の数」または「サイトの耐障害性」の構成 (あるいは両方) が推奨ポリシーと一致しません。 hostAffinity policy option lost during upgrade When you upgrade from vSAN 6.7 to vSAN 8.0, the vCenter Server hostaffinity option value is changed to false . Workaround: Set the hostaffinity option back to true to continue using vSAN HostLocal policy for a normal VM. Cannot enable File Service if vCenter Server internet connectivity is disabled If you disable vCenter Server internet connectivity, the Enable File Service dialog does not display File service agent section and you cannot select OVF. Workaround: To enable vCenter Server internet connectivity: Navigate to Cluster > Configure > vSAN > Internet Connectivity . Click Edit to open Edit Internet Connectivity dialog. Select Enable Internet access for all vSAN clusters checkbox and click Apply . Cannot deactivate encryption on vSAN ESA After you enable data-at-rest encryption on a vSAN ESA cluster, you cannot deactivate it. Workaround: None. vSAN File Service does not support NFSv4 delegations vSAN File Service does not support NFSv4 delegations in this release. Workaround: None. In stretched cluster, file server with no affinity cannot rebalance In the stretched cluster vSAN File Service environment, a file server with no affinity location configured cannot be rebalanced between Preferred ESXi hosts and Non-preferred ESXi hosts. Workaround: Set the affinity location of the file server to Preferred or Non-Preferred by editing the file service domain configuration. Remediation of ESXi hosts in a vSphere Lifecycle Manager cluster with vSAN fails if vCenter services are deployed on custom ports If vCenter Server services are deployed on custom ports in a cluster with vSAN, vSphere DRS, and vSphere HA, remediation of vSphere Lifecycle Manager clusters might fail. This problem is caused by a vSAN resource health check error. ESXi hosts cannot enter maintenance mode, which leads to failing remediation tasks. Workaround: None. When vSAN file service is enabled, DFC-related operations such as upgrade, enabling encryption or data-efficiency might fail When file service is enabled, an agent VM runs on each host. The underlying vSAN object might be placed across multiple diskgroups. When the first diskgroup gets converted, the vSAN object becomes inaccessible and the agent VM is in an invalid state. If you try to delete the VM and redeploy a new VM, the operation fails due to the VM’s invalid state. The VM gets unregistered but the inaccessible object still exists there. When the next diskgroup gets converted, there is a precheck for inaccessible objects in the whole cluster. This check fails the DFC since it finds inaccessible objects of the old agent VM. Workaround: Manually remove the inaccessible objects. When such failure happens, you can see the DFC task failure. Identify the inaccessible objects from the failure task fault information. To ensure that the objects belong to the agent VM, inspect the hostd log file and confirm that the objects belong to the VM’s object layout. Log in to the host and use the /usr/lib/vmware/osfs/bin/objtool command to remove the objects manually. Note : To prevent this problem, disable file service before performing any DFC-related operation. Cannot extract host profile on a vSAN HCI mesh compute-only host vSAN host profile plugin does not support vSAN HCI mesh compute-only hosts. If you try to extract the host profile on an HCI mesh compute-only host, the attempt fails. Workaround: None. Deleting files in a file share might not be reflected in vSAN capacity view The allocated blocks might not be returned back to the vSAN storage instantly after all the files are deleted and hence it would take some time before the reclaimed storage capacity to be updated in vSAN capacity view. When new data is written to the same file share, these deleted blocks might get reused prior to returning them to vSAN storage. If unmap is enabled and vSAN deduplication is disabled, the space may not be freed back to vSAN unless 4MB aligned space are freed in VDFS. If unmap is enabled and vSAN deduplication is enabled, space freed by VDFS will be freed back to vSAN with a delay. Workaround: To release the storage back to vSAN immediately, delete the file shares. vCenter VM crash on stretched cluster with data-in-transit encryption vCenter VM might crash on a vSAN stretched cluster if the vCenter VM is on vSAN with data-in-transit encryption enabled. When all hosts in one site are down and then power on again, the vCenter VM might crash after the failed site returns to service. Workaround: Use the following script to resolve this problem: thumbPrintRepair.py vSAN allows a VM to be provisioned across local and remote datastores vSphere does not prevent users from provisioning a VM across local and remote datastores in an HCI Mesh environment. For example, you can provision one VMDK on the local vSAN datastore and one VMDK on remote vSAN datastore. This is not supported because vSphere HA is not supported with this configuration. Workaround: Do not provision a VM across local and remote datastores. The object reformatting task is not progressing If object reformatting is needed after an upgrade, a health alert is triggered, and vSAN begins reformatting. vSAN performs this task in batches, and it depends on the amount of transient capacity available in the cluster. When the transient capacity exceeds the maximum limit, vSAN waits for the transient capacity to be freed before proceeding with the reformatting. During this phase, the task might appear to be halted. The health alert will clear and the task will progress when transient capacity is available. Workaround: None. The task is working as expected. System VMs cannot be powered-off With the release of vSphere Cluster Services (vCLS) in vSphere 7.0 Update 1, a set of system VMs might be placed within the vSAN cluster. These system VMs cannot be powered-off by users. This issue can impact some vSAN workflows, which are documented in the following article: https://kb.vmware.com/s/article/80877 Workaround: For more information about this issue, refer to this KB article: https://kb.vmware.com/s/article/80483 . vSAN File Service cannot be enabled due to an old vSAN on-disk format version vSAN File Service cannot be enabled with the vSAN on-disk format version earlier than 11.0 (this is the on-disk format version in vSAN 7.0). Workaround: Upgrade the vSAN disk format version before enabling File Service. Host failure in hot-plug scenario when drive is reinserted During a hot drive removal, VMware native NVMe hot-plug can cause a host failure if the NVMe drive is pulled and reinserted within one minute. This is applicable to both vSphere and vSAN for any new or existing drive reinsertion. Workaround: After removing a hot drive, wait for one minute before you reinsert the new or existing drive. Cannot place last host in a cluster into maintenance mode, or remove a disk or disk group Operations in Full data migration or Ensure accessibility mode might fail without providing guidance to add a new resource, when there is only one host left in the cluster and that host enters maintenance mode. This can also happen when there is only one disk or disk group left in the cluster and that disk or disk group is to be removed. Workaround: Before you place the last remaining host in the cluster into maintenance mode with Full data migration or Ensure accessibility mode selected, add another host with the same configuration to the cluster. Before you remove the last remaining disk or disk group in the cluster, add a new disk or disk group with the same configuration and capacity. Object reconfiguration workflows might fail due to the lack of capacity if one or more disks or disk groups are almost full vSAN resyncs get paused when the disks in non-deduplication clusters or disk groups in deduplication clusters reach a configurable resync pause fullness threshold. This is to avoid filling up the disks with resync I/O. If the disks reach this threshold, vSAN stops reconfiguration workflows, such as EMM, repairs, rebalance, and policy change. Workaround: If space is available elsewhere in the cluster, rebalancing the cluster frees up space on the other disks, so that subsequent reconfiguration attempts succeed. After recovery from cluster full, VMs can lose HA protection In a vSAN cluster that has hosts with disks 100% full, the VMs might have a question pending and hence lose the HA protection. Also, the VMs that had a pending question are not HA protected after recovering from cluster full scenario. Workaround: After recovering from vSAN cluster full scenario, perform one of the following actions: Disable and re-enable HA. Reconfigure HA. Power off and power on the VMs. Power Off VMs fails with Question Pending If a VM has a pending question, you are not allowed to do any VM-related operations until the question is answered. Workaround: Try to free the disk space on the relevant volume, and then click Retry . When the cluster is full, the IP addresses of VMs either change to IPV6 or become unavailable When a vSAN cluster is full with one or more disk groups reaching 100%, there can be a VM pending question that requires user action. If the question is not answered and if the cluster full condition is left unattended, the IP addresses VMs might change to IPv6 or become unavailable. This prevents you from using SSH to access the VMs.  It also prevents you from using the VM console, because the console goes blank after you type root . Workaround: None. Unable to remove a dedupe enabled disk group after a capacity disk enters PDL state When a capacity disk in a dedupe-enabled disk group is removed, or its unique ID changes, or when the device experiences an unrecoverable hardware error, it enters Permanent Device Loss (PDL) state. If you try to remove the disk group, you might see an error message informing you that the action cannot be completed. Workaround: Whenever a capacity disk is removed, or its unique ID changes, or when the device experiences an unrecoverable hardware error, wait for a few minutes before trying to remove the disk group. In deduplication clusters, reactive rebalancing might not happen when the disks show more than 80% full In deduplication clusters, when the disks display more than 80% full on the dashboard, the reactive rebalancing might not start as expected. This is because in deduplication clusters, pending writes and deletes are also considered for calculating the free capacity. Workaround: None. TRIM/UNMAP commands from Guest OS fail If the Guest OS attempts to perform space reclamation during online snapshot consolidation, the TRIM/UNMAP commands fail. This failure keeps space from being reclaimed. Workaround: Try to reclaim the space after the online snapshot operation is complete. If subsequent TRIM/UNMAP operations fail, remount the disk. Space reclamation from SCSI TRIM/UNMAP is lost when online snapshot consolidation is performed Space reclamation achieved from SCSI TRIM/UNMAP commands is lost when you perform online snapshot consolidation. Offline snapshot consolidation does not affect SCSI unmap operation. Workaround: Reclaim the space after online snapshot consolidation is complete. Host failure when converting data host into witness host When you convert a vSAN cluster into a stretched cluster, you must provide a witness host. You can convert a data host into the witness host, but you must use maintenance mode with Full data migration during the process. If you place the host into maintenance mode with Ensure accessibility option, and then configure it as the witness host, the host might fail with a purple diagnostic screen. Workaround: Remove the disk group on the witness host and then re-create the disk group. Duplicate VM with the same name in vCenter Server when residing host fails during datastore migration If a VM is undergoing storage vMotion from vSAN to another datastore, such as NFS, and the host on which it resides encounters a failure on the vSAN network, causing HA failover of the VM, the VM might be duplicated in the vCenter Server. Workaround: Power off the invalid VM and unregister it from the vCenter Server. Reconfiguring an existing stretched cluster under a new vCenter Server causes vSAN to issue a health check warning When rebuilding a current stretched cluster under a new vCenter Server, the vSAN cluster health check is red. The following message appears: vSphere cluster members match vSAN cluster members Workaround: Use the following procedure to configure the stretched cluster. Use SSH to log in to the witness host. Decommission the disks on witness host. Run the following command: esxcli vsan storage remove -s ""SSD UUID"" Force the witness host to leave the cluster. Run the following command: esxcli vsan cluster leave Reconfigure the stretched cluster from the new vCenter Server ( Configure > vSAN > Fault Domains & Stretched Cluster ). Disk format upgrade fails while vSAN resynchronizes large objects If the vSAN cluster contains very large objects, the disk format upgrade might fail while the object is resynchronized. You might see the following error message: Failed to convert object(s) on vSAN vSAN cannot perform the upgrade until the object is resynchronized. You can check the status of the resynchronization ( Monitor > vSAN > Resyncing Components ) to verify when the process is complete. Workaround: Wait until no resynchronization is pending, then retry the disk format upgrade. Powered off VMs appear as inaccessible during witness host replacement When you change a witness host in a stretched cluster, VMs that are powered off appear as inaccessible in the vSphere Web Client for a brief time. After the process is complete, powered off VMs appear as accessible. All running VMs appear as accessible throughout the process. Workaround: None. Cannot place hosts in maintenance mode if they have faulty boot media vSAN cannot place hosts with faulty boot media into maintenance mode. The task to enter maintenance mode might fail with an internal vSAN error, due to the inability to save configuration changes. You might see log events similar to the following: Lost Connectivity to the device xxx backing the boot filesystem Workaround: Remove disk groups manually from each host, using the Full data evacuation option . Then place the host in maintenance mode. After stretched cluster failover, VMs on the preferred site register alert: Failed to failover If the secondary site in a stretched cluster fails, VMs failover to the preferred site. VMs already on the preferred site might register the following alert: Failed to failover . Workaround: Ignore this alert. It does not impact the behavior of the failover. During network partition, components in the active site appear to be absent During a network partition in a vSAN two-host or stretched cluster, the vSphere Web Client might display a view of the cluster from the perspective of the non-active site. You might see active components in the primary site displayed as absent. Workaround: Use RVC commands to query the state of objects in the cluster. For example: vsan.vm_object_info Some objects are non-compliant after force repair After you perform a force repair, some objects might not be repaired because the ownership of the objects was transferred to a different node during the process. The force repair might be delayed for those objects. Workaround: Attempt the force repair operation after all other objects are repaired and resynchronized. You can wait until vSAN repairs the objects. When you move a host from one encrypted cluster to another, and then back to the original cluster, the task fails When you move a host from an encrypted vSAN cluster to another encrypted vSAN cluster, then move the host back to the original encrypted cluster, the task might fail. You might see the following message: A general system error occurred: Invalid fault . This error occurs because vSAN cannot re-encrypt data on the host using the original encryption key. After a short time, vCenter Server restores the original key on the host, and all unmounted disks in the vSAN cluster are mounted. Workaround: Reboot the host and wait for all disks to get mounted. Cannot perform deep rekey if a disk group is unmounted Before vSAN performs a deep rekey, it performs a shallow rekey. The shallow rekey fails if an unmounted disk group is present. The deep rekey process cannot begin. Workaround: Remount or remove the unmounted disk group. Log entries state that firewall configuration has changed A new firewall entry appears in the security profile when vSAN encryption is enabled: vsanEncryption. This rule controls how hosts communicate directly to the KMS. When it is triggered, log entries are added to /var/log/vobd.log . You might see the following messages: Firewall configuration has changed. Operation 'addIP4' for rule set vsanEncryption succeeded. Firewall configuration has changed. Operation 'removeIP4' for rule set vsanEncryption succeeded. These messages can be ignored. Workaround: None. HA failover does not occur after setting Traffic Type option on a vmknic to support witness traffic If you set the traffic type option on a vmknic to support witness traffic, vSphere HA does not automatically discover the new setting. You must manually disable and then re-enable HA so it can discover the vmknic. If you configure the vmknic and the vSAN cluster first, and then enable HA on the cluster, it does discover the vmknic. Workaround: Manually disable vSphere HA on the cluster, and then re-enable it. After resolving network partition, some VM operations on linked clone VMs might fail Some VM operations on linked clone VMs that are not producing I/O inside the guest operating system might fail. The operations that might fail include taking snapshots and suspending the VMs. This problem can occur after a network partition is resolved, if the parent base VM's namespace is not yet accessible. When the parent VM's namespace becomes accessible, HA is not notified to power on the VM. Workaround: Power cycle VMs that are not actively running I/O operations. Cannot place a witness host in Maintenance Mode When you attempt to place a witness host in Maintenance Mode, the host remains in the current state and you see the following notification: A specified parameter was not correct . Workaround: When placing a witness host in Maintenance Mode, choose the No data migration option. Moving the witness host into and then out of a stretched cluster leaves the cluster in a misconfigured state If you place the witness host in a vSAN-enabled vCenter cluster, an alarm notifies you that the witness host cannot reside in the cluster. But if you move the witness host out of the cluster, the cluster remains in a misconfigured state. Workaround: Move the witness host out of the vSAN stretched cluster, and reconfigure the stretched cluster. For more information, see this article: https://kb.vmware.com/s/article/2130587 . Unmounted vSAN disks and disk groups displayed as mounted in the vSphere Web Client Operational Status field After the vSAN disks or disk groups are unmounted by either running the esxcli vsan storage disk group unmount command or by the vSAN Device Monitor service when disks show persistently high latencies, the vSphere Web Client incorrectly displays the Operational Status field as mounted. Workaround: Use the Health field to verify disk status, instead of the Operational Status field."
"Before you activate vSAN, verify that your environment meets all requirements. Hardware Requirements for vSAN Verify that the ESXi hosts in your organization meet the vSAN hardware requirements. Cluster Requirements for vSAN Verify that a host cluster meets the requirements for enabling vSAN. Software Requirements for vSAN Verify that the vSphere components in your environment meet the software version requirements for using vSAN. Networking Requirements for vSAN Verify that the network infrastructure and the networking configuration on the ESXi hosts meet the minimum networking requirements for vSAN. License Requirements Verify that you have a valid license for vSAN."
"vSAN introduces specific terms and definitions that are important to understand. Before you get started with vSAN, review the key vSAN terms and definitions. Disk Group A disk group is a unit of physical storage capacity on a host and a group of physical devices that provide performance and capacity to the vSAN cluster. On each ESXi host that contributes its local devices to a vSAN cluster, devices are organized into disk groups. Each disk group must have one flash cache device and one or multiple capacity devices. The devices used for caching cannot be shared across disk groups, and cannot be used for other purposes. A single caching device must be dedicated to a single disk group. In hybrid clusters, flash devices are used for the cache layer and magnetic disks are used for the storage capacity layer. In an all-flash cluster, flash devices are used for both cache and capacity. For information about creating and managing disk groups, see Administering VMware vSAN. Consumed Capacity Consumed capacity is the amount of physical capacity consumed by one or more virtual machines at any point. Many factors determine consumed capacity, including the consumed size of your VMDKs, protection replicas, and so on. When calculating for cache sizing, do not consider the capacity used for protection replicas. Object-Based Storage vSAN stores and manages data in the form of flexible data containers called objects. An object is a logical volume that has its data and metadata distributed across the cluster. For example, every VMDK is an object, as is every snapshot. When you provision a virtual machine on a vSAN datastore, vSAN creates a set of objects comprised of multiple components for each virtual disk. It also creates the VM home namespace, which is a container object that stores all metadata files of your virtual machine. Based on the assigned virtual machine storage policy, vSAN provisions and manages each object individually, which might also involve creating a RAID configuration for every object. When vSAN creates an object for a virtual disk and determines how to distribute the object in the cluster, it considers the following factors: vSAN verifies that the virtual disk requirements are applied according to the specified virtual machine storage policy settings. vSAN verifies that the correct cluster resources are used at the time of provisioning. For example, based on the protection policy, vSAN determines how many replicas to create. The performance policy determines the amount of flash read cache allocated for each replica and how many stripes to create for each replica and where to place them in the cluster. vSAN continually monitors and reports the policy compliance status of the virtual disk. If you find any noncompliant policy status, you must troubleshoot and resolve the underlying problem. Note: When required, you can edit VM storage policy settings. Changing the storage policy settings does not affect virtual machine access. vSAN actively throttles the storage and network resources used for reconfiguration to minimize the impact of object reconfiguration to normal workloads. When you change VM storage policy settings, vSAN might initiate an object recreation process and subsequent resynchronization. See vSAN Monitoring and Troubleshooting. vSAN verifies that the required protection components, such as mirrors and witnesses, are placed on separate hosts or fault domains. For example, to rebuild components during a failure, vSAN looks for ESXi hosts that satisfy the placement rules where protection components of virtual machine objects must be placed on two different hosts, or across fault domains. vSAN Datastore After you enable vSAN on a cluster, a single vSAN datastore is created. It appears as another type of datastore in the list of datastores that might be available, including Virtual Volume, VMFS, and NFS. A single vSAN datastore can provide different service levels for each virtual machine or each virtual disk. In vCenter Server ® , storage characteristics of the vSAN datastore appear as a set of capabilities. You can reference these capabilities when defining a storage policy for virtual machines. When you later deploy virtual machines, vSAN uses this policy to place virtual machines in the optimal manner based on the requirements of each virtual machine. For general information about using storage policies, see the vSphere Storage documentation. A vSAN datastore has specific characteristics to consider. vSAN provides a single vSAN datastore accessible to all hosts in the cluster, whether or not they contribute storage to the cluster. Each host can also mount any other datastores, including Virtual Volumes, VMFS, or NFS. You can use Storage vMotion to move virtual machines between vSAN datastores, NFS datastores, and VMFS datastores. Only magnetic disks and flash devices used for capacity can contribute to the datastore capacity. The devices used for flash cache are not counted as part of the datastore. Objects and Components Each object is composed of a set of components, determined by capabilities that are in use in the VM Storage Policy. For example, with Primary level of failures to tolerate set to 1, vSAN ensures that the protection components, such as replicas and witnesses, are placed on separate hosts in the vSAN cluster, where each replica is an object component. In addition, in the same policy, if the Number of disk stripes per object configured to two or more, vSAN also stripes the object across multiple capacity devices and each stripe is considered a component of the specified object. When needed, vSAN might also break large objects into multiple components. A vSAN datastore contains the following object types: VM Home Namespace The virtual machine home directory where all virtual machine configuration files are stored, such as .vmx , log files, vmdks, and snapshot delta description files. VMDK A virtual machine disk or .vmdk file that stores the contents of the virtual machine's hard disk drive. VM Swap Object Created when a virtual machine is powered on. Snapshot Delta VMDKs Created when virtual machine snapshots are taken. Memory object Created when the snapshot memory option is selected when creating or suspending a virtual machine. Virtual Machine Compliance Status: Compliant and Noncompliant A virtual machine is considered noncompliant when one or more of its objects fail to meet the requirements of its assigned storage policy. For example, the status might become noncompliant when one of the mirror copies is inaccessible. If your virtual machines are in compliance with the requirements defined in the storage policy, the status of your virtual machines is compliant. From the Physical Disk Placement tab on the Virtual Disks page, you can verify the virtual machine object compliance status. For information about troubleshooting a vSAN cluster, see vSAN Monitoring and Troubleshooting. Component State: Degraded and Absent States vSAN acknowledges the following failure states for components: Degraded. A component is Degraded when vSAN detects a permanent component failure and determines that the failed component cannot recover to its original working state. As a result, vSAN starts to rebuild the degraded components immediately. This state might occur when a component is on a failed device. Absent. A component is Absent when vSAN detects a temporary component failure where components, including all its data, might recover and return vSAN to its original state. This state might occur when you are restarting hosts or if you unplug a device from a vSAN host. vSAN starts to rebuild the components in absent status after waiting for 60 minutes. Object State: Healthy and Unhealthy Depending on the type and number of failures in the cluster, an object might be in one of the following states: Healthy. When at least one full RAID 1 mirror is available, or the minimum required number of data segments are available, the object is considered healthy. Unhealthy. An object is considered unhealthy when no full mirror is available or the minimum required number of data segments are unavailable for RAID 5 or RAID 6 objects. If fewer than 50 percent of an object's votes are available, the object is unhealthy. Multiple failures in the cluster can cause objects to become unhealthy. When the operational status of an object is considered unhealthy, it impacts the availability of the associated VM. Witness A witness is a component that contains only metadata and does not contain any actual application data. It serves as a tiebreaker when a decision must be made regarding the availability of the surviving datastore components, after a potential failure. A witness consumes approximately 2 MB of space for metadata on the vSAN datastore when using on-disk format 1.0, and 4 MB for on-disk format for version 2.0 and later. vSAN 6.0 and later maintains a quorum by using an asymmetrical voting system where each component might have more than one vote to decide the availability of objects. Greater than 50 percent of the votes that make up a VM’s storage object must be accessible at all times for the object to be considered available. When 50 percent or fewer votes are accessible to all hosts, the object is no longer accessible to the vSAN datastore. Inaccessible objects can impact the availability of the associated VM. Storage Policy-Based Management (SPBM) When you use vSAN, you can define virtual machine storage requirements, such as performance and availability, in the form of a policy. vSAN ensures that the virtual machines deployed to vSAN datastores are assigned at least one virtual machine storage policy. When you know the storage requirements of your virtual machines, you can define storage policies and assign the policies to your virtual machines. If you do not apply a storage policy when deploying virtual machines, vSAN automatically assigns a default vSAN policy with Primary level of failures to tolerate set to 1, a single disk stripe for each object, and thin provisioned virtual disk. For best results, define your own virtual machine storage policies, even if the requirements of your policies are the same as those defined in the default storage policy. For information about working with vSAN storage policies, see Administering VMware vSAN. vSphere PowerCLI VMware vSphere PowerCLI adds command-line scripting support for vSAN, to help you automate configuration and management tasks. vSphere PowerCLI provides a Windows PowerShell interface to the vSphere API. PowerCLI includes cmdlets for administering vSAN components. For information about using vSphere PowerCLI, see vSphere PowerCLI Documentation."
"VMware vSAN is a distributed layer of software that runs natively as a part of the ESXi hypervisor. vSAN aggregates local or direct-attached capacity devices of a host cluster and creates a single storage pool shared across all hosts in the vSAN cluster. While supporting VMware features that require shared storage, such as HA, vMotion, and DRS, vSAN eliminates the need for external shared storage and simplifies storage configuration and virtual machine provisioning activities. vSAN Concepts VMware vSAN uses a software-defined approach that creates shared storage for virtual machines. It virtualizes the local physical storage resources of ESXi hosts and turns them into pools of storage that can be divided and assigned to virtual machines and applications according to their quality-of-service requirements. vSAN is implemented directly in the ESXi hypervisor. vSAN Terms and Definitions vSAN introduces specific terms and definitions that are important to understand. vSAN and Traditional Storage Although vSAN shares many characteristics with traditional storage arrays, the overall behavior and function of vSAN is different. For example, vSAN can manage and work only with ESXi hosts and a single vSAN instance can support only one cluster. Building a vSAN Cluster If you are considering vSAN, you can choose from more than one configuration solution for deploying a vSAN cluster. vSAN Deployment Options This section covers the different supported deployment options that are supported for vSAN clusters. Integrating with Other VMware Software After you have vSAN up and running, it is integrated with the rest of the VMware software stack. You can do most of what you can do with traditional storage by using vSphere components and features including vSphere vMotion, snapshots, clones, Distributed Resource Scheduler (DRS), vSphere High Availability, vCenter Site Recovery Manager, and more. Limitations of vSAN This topic discusses the limitations of vSAN."
"Verify that the network infrastructure and the networking configuration on the ESXi hosts meet the minimum networking requirements for vSAN. Networking Requirements for vSAN Networking Component Requirement Host Bandwidth Each host must have minimum bandwidth dedicated to vSAN. Dedicated 1 Gbps for hybrid configurations Dedicated or shared 10 Gbps for all-flash configurations For information about networking considerations in vSAN, see Designing the vSAN Network . Connection between hosts Each host in the vSAN cluster, regardless of whether it contributes capacity, must have a VMkernel network adapter for vSAN traffic. See Set Up a VMkernel Network for vSAN . Host network All hosts in your vSAN cluster must be connected to a vSAN Layer 2 or Layer 3 network. IPv4 and IPv6 support The vSAN network supports both IPv4 and IPv6. Network latency Maximum of 1 ms RTT for standard (non-stretched) vSAN clusters between all hosts in the cluster Maximum of 5 ms RTT between the two main sites for stretched clusters Maximum of 200 ms RTT from a main site to the vSAN witness host"
"Although vSAN shares many characteristics with traditional storage arrays, the overall behavior and function of vSAN is different. For example, vSAN can manage and work only with ESXi hosts and a single vSAN instance can support only one cluster. vSAN and traditional storage also differ in the following key ways: vSAN does not require external networked storage for storing virtual machine files remotely, such as on a Fibre Channel (FC) or Storage Area Network (SAN). Using traditional storage, the storage administrator preallocates storage space on different storage systems. vSAN automatically turns the local physical storage resources of the ESXi hosts into a single pool of storage. These pools can be divided and assigned to virtual machines and applications according to their quality-of-service requirements. vSAN does not behave like traditional storage volumes based on LUNs or NFS shares. The iSCSI target service uses LUNs to enable an initiator on a remote host to transport block-level data to a storage device in the vSAN cluster. Some standard storage protocols, such as FCP, do not apply to vSAN. vSAN is highly integrated with vSphere. You do not need dedicated plug-ins or a storage console for vSAN, compared to traditional storage. You can deploy, manage, and monitor vSAN by using the vSphere Client. A dedicated storage administrator does not need to manage vSAN. Instead a vSphere administrator can manage a vSAN environment. With vSAN, VM storage policies are automatically assigned when you deploy new VMs. The storage policies can be changed dynamically as needed."
"Verify that the vSphere components in your environment meet the software version requirements for using vSAN. To use the full set of vSAN capabilities, the ESXi hosts that participate in vSAN clusters must be version 7.0 Update 1 or later. During the vSAN upgrade from previous versions, you can keep the current on-disk format version, but you cannot use many of the new features. vSAN 7.0 Update 1 and later software supports all on-disk formats."
"After you have vSAN up and running, it is integrated with the rest of the VMware software stack. You can do most of what you can do with traditional storage by using vSphere components and features including vSphere vMotion, snapshots, clones, Distributed Resource Scheduler (DRS), vSphere High Availability, vCenter Site Recovery Manager, and more. Integrating with vSphere HA You can enable vSphere HA and vSAN on the same cluster. As with traditional datastores, vSphere HA provides the same level of protection for virtual machines on vSAN datastores. This level of protection imposes specific restrictions when vSphere HA and vSAN interact. For specific considerations about integrating vSphere HA and vSAN, see Using vSAN and vSphere HA . Integrating with VMware Horizon View You can integrate vSAN with VMware Horizon View. When integrated, vSAN provides the following benefits to virtual desktop environments: High-performance storage with automatic caching Storage policy-based management, for automatic remediation For information about integrating vSAN with VMware Horizon, see the VMware Horizon with View documentation. For designing and sizing VMware Horizon View for vSAN, see the Designing and Sizing Guide for Horizon View."
"Verify that a host cluster meets the requirements for enabling vSAN. All capacity devices, drivers, and firmware versions in your vSAN configuration must be certified and listed in the vSAN section of the VMware Compatibility Guide. A standard vSAN cluster must contain a minimum of three hosts that contribute capacity to the cluster. A two host vSAN cluster consists of two data hosts and an external witness host. For information about the considerations for a three-host cluster, see Design Considerations for a vSAN Cluster . A host that resides in a vSAN cluster must not participate in other clusters."
"Verify that you have a valid license for vSAN. Using vSAN in production environments requires a special license that you assign to the vSAN clusters. You can assign a standard vSAN license to the cluster, or a license that covers advanced functions. Advanced features include RAID 5/6 erasure coding, and deduplication and compression. An enterprise license is required for encryption and stretched clusters. For information about assigning licenses, see Configure License Settings for a vSAN Cluster . The capacity of the license must cover the total number of CPUs in the cluster."
